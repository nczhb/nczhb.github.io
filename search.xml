<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[信用评分卡(浓缩版)]]></title>
    <url>%2F2018%2F11%2F09%2Fcredit-card-sum%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;信用评分卡通常分为四种：A卡、B卡、C卡和F卡，其中B卡也称行为评分卡，通过对已经通过审批并进行执行阶段的账户进行评分，预测客户开户后一定时期内违约拖欠的风险概率，排除信用不良用户和非目标客户的贷款申请&emsp;&emsp;下面我就用kaggle上一个项目的数据来简单做一个行为评分卡。&emsp;&emsp;详情步骤和代码可以看后面的3篇博客 思路&emsp;&emsp;数据集来自kaggle的give me some credit项目，原项目要求我们根据用户信息和行为指标预测是否坏用户，我则在对用户分类的基础上继续对用户进行评分。&emsp;&emsp;分类模型我选择LR，解释力强，同时模型的参数结合变量的woe值就能生成一个评分系统，简单高效&emsp;&emsp;通常由于申请时很多用户未填入准确的信息，用户数据会存在很多缺失和异常值，需要根据不同情况进行清洗和处理&emsp;&emsp;用户数据很不平衡达到14:1，毕竟真正信用较差的用户还是很少，但很多算法的基本假设就是数据分布是均匀或正态的，因此对数据不平衡的处理也是一个重点&emsp;&emsp;变量需要进行卡方分箱，并用分箱的woe值替换原数据作为新变量&emsp;&emsp;由于知识有限，目前先做到给出一个初步评分。 观察数据和清洗 大体观察&emsp;&emsp;数据全部都是连续型数值，可分为个人基本信息（年龄、家庭成员数）、经济水平（月收入、负债率）、行为信息（额度使用率、不同程度的逾期、贷款的类型和数量），当然还有目标变量，值域为(0,1)&emsp;&emsp;通过计算各个变量间相关系数，可以看到3个不同程度逾期的变量间相关程度在0.98以上，属于冗余变量，对模型没有提升但带来更多的运算量，所以只保留和目标特征相关系数最大的一个&emsp;&emsp;除此之外，只有NumberRealEstateLoansOrLines和NumberOfOpenCreditLinesAndLoans有中等水平的相关，前者表示抵押贷款和不动产借款数量，后者为open loans和信用卡贷款 数量，两者在内容上有部分交集，这也给后面创建新变量提供了一个思路，其它变量之间相关系数较低。 异常和缺失&emsp;&emsp;观察各个变量的分布我们可以看到很多异常的值：&emsp;&emsp;1、有一个样本年龄填0，并且在训练集里，直接删除该样本&emsp;&emsp;2、debtratio变量为还款、赡养费和生活开销占月毛收入的比例，有超过1/5的样本这个变量的值大于1，并且绝大部分的值都很大，在百分位25的位置就已经到42，这些数据一定存在问题。&emsp;&emsp;如果观察它和MIncome的关系，MIncome缺失的样本几乎DebtRatio都大于1，并且这些样本占所有Debtratio&gt;1的样本超过80%，剩下debtratio较高的样本，绝大部分是MIncome较低，随着MIncome增加，大于1的Debtratio越来越少。 &emsp;&emsp;对于MIncome缺失的部分，我认为可能是系统为了能够计算Debtratio而强行设置为1导致的，而MIcome较小的部分，具体原因不能确定，但应该有部分是非真实数据。&emsp;&emsp;所以对于MIncome较低的样本，之后使用卡方分箱，看是否能分出几个部分，而对MIncome缺失的样本，由于缺失量较大，约占总体的1/5，所以直接作为一类&emsp;&emsp;对于Debtratio&gt;1的样本，也通过卡方分箱，如果能分出几个比较显著的组(和相邻组卡方值较大)则将他们分成几部分，否则也统一作为一类&emsp;&emsp;&emsp;&emsp;3、除了MIncome，还有一个变量Dependents也存在缺失值，缺失的样本量不大，且这个变量和age、NumberRealEstateLoansOrLines存在弱相关，所以考虑使用random forest根据其它的变量来拟合缺失了的Dependents&emsp;&emsp;在拟合过程中，我去掉了原来MIncome缺失的样本，因为这些人里可能有些有意不填收入，那么他们数据的真实性可能和其他人的数据有差距12345678rf_train_x = train[train['MIncome']!=-10000].iloc[:, [1,2,3,4,6,7]]rf_train_y = train[train['MIncome']!=-10000]['Dependents']from sklearn.ensemble import RandomForestRegressorclf = RandomForestRegressor(random_state=0, n_estimators=500, n_jobs=-1)clf.fit(rf_test_x, rf_test_y)clf.score(rf_test_x, rf_test_y)# -&gt; 0.87447288413972102 新建变量&emsp;&emsp;在前面的处理过程中我们就能看到有两组变量间有关联，一个是相互有交集的NumberRealEstateLoansOrLines和NumberOfOpenCreditLinesAndLoans，还有就是Debtratio和MIncome。&emsp;&emsp;前面一对，似乎后者的openloans已经大部分包含了前者，所以我考虑用后者减去前者作为一个新变量，这个变量可能和信用卡贷款有关。123456#这一步我是在smote后做的resample_train['credit'] = resample_train['OpenL2'] - resample_train['REL']test['credit'] = test['OpenL2'] - test['REL']resample_train.to_csv('train_fixed1.csv', index=False)test.to_csv('test_fixed1.csv', index=False) &emsp;&emsp;后面一对可以求出debt的值，造出这个变量相对于Debtratio的意义在于测试一下是否有钱人借的钱多了也爱不还，如果不是这样，那么这个变量就没什么效果可以删除了12345pos_train_Income = resample_train['MIncome'].replace([0, -10000], 1)resample_train['Debt'] = pos_train_Income * resample_train['DebtRatio']pos_test_Income = test['MIncome'].replace([0, -10000], 1)test['Debt'] = pos_test_Income * test['DebtRatio'] 数据不平衡&emsp;&emsp;接下来讨论一下数据不平衡的问题，通常来说如果数据量很大就可以考虑做下采样，但这里明显不适合；而上采样又容易过拟合，所以我采用上采样和下采样结合的方式。首先用smote将正负样本比例升为10:1，之后不直接降采样，而是降学习分为n次进行，每次学习抽取所有的负样本和部分正样本，最后把学习的结果求一个平均值，既起到了sampling的效果降低过拟合，又能降低数据的不平衡性&emsp;&emsp;所以这里先做一个somte，smote函数借鉴简书上一位同学的写法，具体代码见详情版1resample_train = smote(train, tag_index=0, std_rate=10, kdistinctvalue=8) 变量分箱和筛选&emsp;&emsp;经过前面的工作，对缺失和异常值以及数据的分布做了初步的处理，接下来对变量进行分箱，分箱后每组的值用其woe值代替，然后对新生成的变量进行评估和筛选&emsp;&emsp;卡方分箱的函数是我参照一篇论文写的，逻辑是对一个排好序的变量，对相邻两个unique值及其对应的y做卡方检验，取里面卡方值最小的一对，如果依然可以达到某个显著水平，例如p值&lt;=0.05，则保留这两个小组，结束分箱，否则将他们合并，不断循环这个过程直到停止。具体代码见详情版。&emsp;&emsp;但由于有些变量连续数据的unique值太多，直接套用这个函数运算量太大且没必要，所以我先手动把这些数值划分成很多小段，例如前面的Debtratio我把它在0-1范围分成100段，这样对结果影响很小，但分箱快了很多。&emsp;&emsp;我在分好箱后就直接求woe值和iv值，并存入字典方便后续调用，所以先定义一下计算的函数12345678910111213141516171819202122232425def good(x): return (len(x)-sum(x))/good_sumdef bad(x): return sum(x)/bad_sumbad_sum = int(train['target'].sum())good_sum = len(train['target']) - bad_sumivs = &#123;&#125;def woe_iv(var, cut): data = Series(train['target'].values, train[var].values) data.index = pd.cut(data.index, cut, right=False) print(data.groupby(data.index).count()) ratio = data.groupby(data.index).agg([good,bad]) woe = np.log(ratio['good']/ratio['bad']) sub = ratio['good']-ratio['bad'] iv = (sub*woe).sum() ivs[var] = iv print('woe:') print(woe, '\n') print('iv:', iv) train[var] = pd.cut(train[var], cut, right=False).map(woe.to_dict()) test[var] = pd.cut(test[var], cut, right=False).map(woe.to_dict()) print(woe.to_dict()) &emsp;&emsp;下面展示对age变量的分箱：12345678910111213141516171819202122232425262728293031323334353637freq_age = pd.crosstab(train.age, train.target)merge(freq_age)-&gt; Int64Index([21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99], dtype='int64', name='age') 21.763776 [(0, 1)] 32.122824 [(1, 2)] 12.092637 [(2, 3)] 24.292766 [(3, 4)] 23.000035 [(4, 5)] 34.589954 [(5, 6)] 14.700949 [(6, 7)] 11.499217 [(7, 8)] 133.779114 [(8, 9)] 18.759893 [(9, 10)] 58.973590 [(10, 11)] 9.492844 [(11, 12)] 12.886260 [(12, 13)] dtype: objectwoe_iv('age', [21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99, 1000])-&gt; woe: [21, 23) 0.536632 [23, 25) -0.172970 [25, 33) -0.569851 [33, 37) -0.462247 [37, 48) -0.328169 [48, 54) -0.227933 [54, 57) -0.048145 [57, 58) 0.166081 [58, 63) 0.352834 [63, 66) 0.897501 [66, 73) 1.137055 [73, 84) 1.627105 [84, 99) 2.043198 [99, 1000) 0.083389 dtype: float64 iv: 0.354380135746 &emsp;&emsp;经过分箱，age被分为了14段，因为不存在一个年龄段里包含绝大多数样本，所以不用考虑拆分，之后可以根据模型的泛化能力适当对某些分箱继续合并，但先维持这样。&emsp;&emsp;分箱后age变量的iv值为0.35，具有一定的预测力，适合保留。 &emsp;&emsp;有一些变量的iv值不高不低，我们可以结合其它feature selection方式综合考虑。这里我使用xgboost的plot_importance，它可以利用self-validation时的oob值来做特征选择123456from xgboost import XGBClassifierclf = XGBClassifier()clf.fit(train.iloc[:,[0,1,2,3,4,5,6,7,9,10]], train['target'])from xgboost import plot_importanceplot_importance(clf)plt.show() &emsp;&emsp;下面是结合iv值和xgboost特征选择的结果iv&gt;1 f_name iv RuOfUL 1.84 额度使用率越高失信率越高 30-59DaysPastDue 1.64 老赖和失信是扯不开的 &emsp;&emsp;虽然iv值大于1有过于美好的怀疑，但是这两个变量的预测力从实际角度来看也解释得通，所以还是保留下来0.3&lt;iv&lt;0.5 f_name iv age 0.354 解释力良好，中年群体的失信率相对较高 0.1&lt;0v&lt;0.3 f_name iv debtratio 0.18 0-0.4时debtratio越高失信率越高，但0.4-1变化就不明显了，大于1的就不解释了 REL(淘汰) 0.13 本身就iv值较低，xbgoost对它的评价也很低，就抛弃这个变量了 dependents 0.24 家庭成员越多，失信率越低 MIncome 0.09 2500左右工资的人失信率高一些，除此之外还是可以看到收入越高失信率越低，只是变化幅度较小，对于iv值介于0.05到0.1的变量，可以酌情考虑保留 &emsp;&emsp;还有一些变量因为iv值过低直接排除掉，包括之前单独造出的两个变量。 &emsp;&emsp;做好特征工程后，就要开始建立模型了。&emsp;&emsp;由于我后面要对数据分批进行学习，用交叉验证会麻烦一点，所以我直接使用留出法，70%的数据用来学习，30%的数据用于验证模型的拟合度 12X_train, X_test, y_train, y_test = train_test_split( train.iloc[:,:-1], train['target'], test_size=0.3, random_state=42) 建模数据分组&emsp;&emsp;逻辑之前已经提过了，就是对负样本下采样(每次随机取一半)然后和正样本拼接，我通过尝试发现取7组出来时最终拟合效果最好。1234567891011def undersample(X, y, subsets=7): data = X.join(y) train_sets = [] neg = data[data['target']==0] pos = data[data['target']==1] for i in range(subsets): re_neg = neg.sample(frac=0.5) combine = pd.concat([re_neg, pos]) train_sets.append(combine) return train_setstrain_sets = undersample(X_train, y_train, subsets=7) 定义学习函数&emsp;&emsp;使用LR模型，l2系数设置为2，分别对每组数据学习，然后把模型的系数、截距和预测值存入一个字典并返回12345678910111213141516# LR的c=2是我之前对7组数据分别验证，找到的拟合分数平均数较高、标准差较低的c值，实际上这几组数据的训练结果差别不大def training(data_sets, test): test_results = [] coefs = [] intercepts = [] model = LogisticRegression(C=2) for data in data_sets: y_train = data['target'] x_train = data.iloc[:,:-1] clf = model.fit(x_train.values, y_train.values) coefs.append(clf.coef_) intercepts.append(clf.intercept_) test_results.append(clf.predict_proba(test)[:,1]) return &#123;'test_results':test_results, 'coefs':coefs, 'intercepts':intercepts&#125; roc曲线ks曲线&emsp;&emsp;auc值为0.88，ks值为0.61，并且考虑到训练集的数据分布和测试集的不同，模型泛化能力还行，可以把其它数据也放进来学习。 使用所有的数据进行学习1234train_sets = undersample(train.iloc[:,:-1], train['target'], subsets=7)training_result = training(train_sets, test.iloc[:, :-1])intercept = DataFrame(training_result['intercepts']).mean()[0]coefs = DataFrame(np.array(training_result['coefs'])[:,0,:]).mean() &emsp;&emsp;收集模型的系数和截距并对它们求均值，以供后面评分使用 评分&emsp;&emsp;评分根据违约和正常的概率比和我们期望对应的分数建立一个线性方程。&emsp;&emsp;这个比例我们用Odds表示&emsp;&emsp;评分就是关于Odds的线性函数&emsp;&emsp;其中log(Odds)就是p套用sigmoid的反函数，就是x(woe)向量和LR系数向量的乘积&emsp;&emsp;而确定A和B的值，通常是假设当比率为Θ时分数为P，比率为2Θ时分数为P+PDO &emsp;&emsp;这里我选择Θ=1/20，P=600， PDO=-30，则：12b = 30/np.log(2)a = 600 + b*np.log(odd) &emsp;&emsp;最终得到分数：12345678score = \a-b*intercept-\-0.783247*b*test['RuOfUL'] -\-0.382026*b*test['age'] -\-0.720417*b*test['30-59DaysPastDue'] -\-0.182033*b*test['DebtRatio'] -\-0.543901*b*test['MIncome'] -\-0.603020*b*test['Dependents'] &emsp;&emsp;使用这种方法对用户进行评分，我个人的感觉是，方便高效可解释性高。首先它不需要对每个新的变量做一个prediction，只要根据用户特征值做个简单的加法乘法就可以得到分数，可以大量减少计算资源，并且依然可以将用户有效区分开；其次，LR本身就是一个高效方便解释的模型，制作成评分卡后对用户的信用评估的过程更容易让人理解，为以后对模型的检测和报告都提供了便利。 &emsp;&emsp;建立模型后，我们对tset数据集的用户做一个评分，并观察这些用户的评分分布&emsp;&emsp;用户的分数分布在350-700之间，呈一个左偏的分布。这个直方图有三座山峰，感觉还挺符合现状的，将近一半人评分大于550，属于信用良好客户；其次是分数在450-550，属于普通用户，这些人可能存在失信的风险；剩下最少的是450分以下的属于准老赖用户，失信概率较高。 &emsp;&emsp;实际中的评分卡除了上述步骤，还需要添加信用风险标准评分的结果，两者综合才是最终得分。&emsp;&emsp;除此之外，我们需要制作一个刻度，例如将分数从高到低排列，切分成几部分，使得各个区间失信率呈某种分布，最终给这几个区间评一个级别，这样就可以根据分数知道用户具体等级&emsp;&emsp;之后就是对模型的监控和更新，保证模型能随着用户的变化而适应实际业务&emsp;&emsp;这些具体的操作，因为不了解行业内具体标准，所以就不进行下去了，反正肯定没我想的这么简单。]]></content>
  </entry>
  <entry>
    <title><![CDATA[拉钩网数据分析师岗位分析]]></title>
    <url>%2F2018%2F11%2F09%2Flagou%2F</url>
    <content type="text"><![CDATA[div#salary{ width:100%; height:30vw; margin:auto; } div#flex_h{ display: flex; justify-content:center; width: 80%; position:relative; left: 10%; margin: 2px; } div#flex_v{ display: flex; flex-direction: column; width: 100%; } div.echarts{ width: 100%; height: 30vw; margin:auto; } div#comment1{ width: 100%; margin:auto; } div#comment2{ width: 100%; margin:auto; } button{ background-color: white; border: 2px solid #555555; margin: 4px 2px; } button:hover { background-color: #555555; color: white; } img { width: 70%;} 前言&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;之前因为需要抉择去广州还是深圳找工作，爬取了一些数据，顺便做了个简单的数据分析，旨在了解数据分析岗位的信息以及两个城市间的差别，我会尽量少说废话，多用图来展示信息，里面应该会有一些不准确的理解和推测，所以只供参考，更多的还是让数据自己说话&emsp;&emsp;&emsp;&emsp;本来之前就打算找工作，但是因为一些事情我现在才开始去找工作，所以数据有点早，是8月份的数据&emsp;&emsp;&emsp;&emsp; 分析过程 使用python爬取拉勾网关于数据分析的岗位信息，并存入数据库 使用selenium爬取职位信息以及详情网页，通过requests库爬取具体信息，并通过高德地图api获取公司经纬度（拉勾网的地理位置就是用高德地图定位的） 使用BeautifulSoup库解析html 调用mysql的api将数据存入数据库 通过numpy和pandas对数据进行各类描述性统计分析 对关键词使用python的WordCloud包生成词云，需要用到jieba分词 词云我简单做成了方形，是有点丑。。。 使用百度的echarts对数据进行可视化 对中间有一部分的图，因为比较多，所以我把他们放在一个div里，通过点击按钮来切换，节省空间，观感也更好 下面开始展示分析结果： 公司地理分布及其岗位工资&emsp;&emsp;将两个城市提供数据分析岗位的公司地理位置在地图上标出，并通过颜色反映相应工资：&emsp;&emsp;紫色波浪边缘的点表示该城市工资排名前十的公司，其余点根据颜色表示岗位工资水平，越接近红色越高，越接近绿色越低&emsp;&emsp;将鼠标悬浮在左下角的单位标记上的某个位置从而只显示相应工资水平的公司位置，也可以拖动调整其上下界，来限制只显示某个范围工资的公司&emsp;&emsp;将鼠标悬浮在地图上的点或行政区上，从而查看该公司提供岗位的工资或该行政区的平均工资&emsp;&emsp;可以鼠标拖动或滚轮缩放地图 &emsp;&emsp;在深圳绝大部分公司分布在南山区和福田区，平均工资也是前2,分别为18.54、19.46，远高于其他地区并且工资top10的公司也都在这里&emsp;&emsp;其次罗湖区和宝安区也有少量分布，剩余地方就只有零零星星的岗位了&emsp;&emsp;看来如果去深圳找工作，主要就是在南山区和福田区了&emsp;&emsp;在广州，天河区的岗位数量最多，但是平均工资13.06，标准差较大&emsp;&emsp;其次越秀区、荔湾区、海珠区、番禺区、黄埔区也有一定数量的岗位，并且荔湾、海珠、番禺的平均工资都比天河区高，越秀区平均工资最低，只有10.34，黄埔区紧追其次平均工资12.38&emsp;&emsp;白云区只有三个岗位，平均工资15.75&emsp;&emsp;工资前十的公司有6个在天河，1个在越秀，2个在荔湾，1个在番禺&emsp;&emsp;感觉天河承包了广州数据分析的半壁江山薪资分布&emsp;&emsp;&emsp;&emsp;对所有岗位的薪资保留2位小数，然后用曲线图描述两个城市在各个薪资水平上的分布数量&emsp;&emsp;深圳20k以上的岗位比广州多，20k以下差别不明显，所以两个城市薪资大体分布接近&emsp;&emsp;两个城市的工资肉眼上可以分出四档：&emsp;&emsp;&emsp;&emsp; 5k-15k&emsp;&emsp;&emsp;&emsp; 20k-23k&emsp;&emsp;&emsp;&emsp; 30k左右&emsp;&emsp;&emsp;&emsp; 40k左右&emsp;&emsp;第一档占了大部分的岗位，也是大部分人入行的方向&emsp;&emsp;第二档和第一档之间有一个明显的空档，所以应该对技能和经验有高一阶的要求&emsp;&emsp;第三档和第四档可能是给管理和项目负责人的岗位&emsp;&emsp;工资可以很直接地反映对工作能力的要求，这四个阶段应该对应了入门-&gt;晋级-&gt;资深-&gt;大牛。看来上岗之后还有很多修行路等着我们岗位信息及其与薪资关系&emsp;&emsp;一个岗位基本的信息包括经验要求、学历要求、所处行业、公司规模和融资情况，接下来将展示两个城市里这5个特征的分布比例，以及他们和薪资之间的关系&emsp;&emsp;通过点击方框来切换要查看的特征，上下两排的按钮效果是一样的经验学历行业规模融资 12 34 经验学历行业规模融资 数据分析岗位关键词&emsp;&emsp;两个城市关于数据分析岗位的关键词以及各自占比比较接近，所以把两个城市的数据合并到一起，让我们一起看一下关于这个岗位有哪些关键词吧～主要关键词：大数据、数据挖掘、金融、带薪年假、绩效奖金、spss、sas、建模、电商、运营、节日礼物等 公司关键词&emsp;&emsp;让我们来看看这些公司主要怎么展示自己的&emsp;&emsp;大概都是薪酬待遇福利、发展前景好，平台大，领导同事牛逼，团队氛围好之类的意思 任职要求关键词&emsp;&emsp;这些是从一大堆任职要求描述里提取的关键词&emsp;&emsp;主要关键词有数据分析、业务、模型、产品、工作经验等&emsp;&emsp;对于我来说，工作经验 优先、相关专业这几个词虽然不大但是异常的刺眼 技能要求词云深圳 广州 &emsp;&emsp;两个城市对工具的需求有点不一样&emsp;&emsp;在广州，excel和sql占了两个很大比例，比其他工具要多不少&emsp;&emsp;但在深圳各个基本工具都占有较大比例，或者说均衡一点，例如python、r、sas、sql、excel，并且python和sql占比最大 &emsp;&emsp;按照’热度’来说，总体排名前10的工具有sql、python、excel、r、sas、spss、hadoop、hive、ppt、spark&emsp;&emsp;看来除了基本的office办公和sql，懂得一门语言应该是标配，能使用分布式工具就更好了 &emsp;&emsp;接下来我们将根据技能展开一些分析，主要是看要求这些技能的岗位的平均工资以及尝试探索6个主要行业对这些技能的需求度技能与薪资&emsp;&emsp;现在我们来看一下这些工具和工资的关系，感受一下它们各自的’价值’&emsp;&emsp;如果想看单个城市的图，可以点相应城市的标签，然后取消”两城市平均”的标签 &emsp;&emsp;两个城市在技能-薪资分布大体比较接近，所以合并到一起求了个平均值，但是要注意，广州的工资普遍要比深圳低3-5k&emsp;&emsp;粗略来看，根据行业工资可以分成3档：&emsp;&emsp;1档： excel、ppt&emsp;&emsp;2档： tableau、spss、mysql、oracle、sql、sas、r、python&emsp;&emsp;3档： hive、linux、java、hadoop、spark&emsp;&emsp;可以感性地判断，excel和ppt是最基础，其次是掌握sql和一门分析语言，最好能做出效果好的可视化，如果想进阶可以学习分布式工具，除此之外linux和java是很好的加分项&emsp;&emsp;不过两个城市还是有一些差别：&emsp;&emsp;深圳的spss工资接近sas、r、python，达到19.7，但是广州的spss就较低，平均13k&emsp;&emsp;广州具体提及sql类型的，例如mysql和oracle的工资较高，一个17.06一个17.96，通过查看对应的任职要求，发现广州明确指出sql类型的，大多写的比较规范，也要求较多是一些知名公司，但是这个情况到深圳就好像不存在了&emsp;&emsp;广州似乎还很看重可视化，要求tableau的岗位工资平均16.36，比python、r这些还要高1k 各行业技能需求&emsp;&emsp;在各个行业里，排名前六的工具是一样的，可以算是行业通用技能了，所以接下来对比一下各个行业对这些技能需求的差别：&emsp;&emsp;分别统计这6个行业里，这些技能在所有技能里的’提及率’(该工具标签出现频率)&emsp;&emsp;提及率越高，说明可能这个行业可能更依赖这个工具，但也不绝对，仅供参考&emsp;&emsp;如果这几个工具的提及率都一般，说明这个行业可能要求使用的工具更杂 &emsp;&emsp;对每个行业来说，sql应该属于必备工具了，提及率都是最高的&emsp;&emsp;和其他行业差别最大的就是金融行业，sas的提及率比其他行业高了很多，金融行业使用sas多可以说是传统了，除此之外其他工具的提及率在其他行业里也算中上水平，其中r、spss、sql在这6个行业里占比最高&emsp;&emsp;第二类是电子商务和企业服务，sql和excel的提及率较其他工具高，电子商务的python、spss和r在所有行业里提及率最低&emsp;&emsp;剩下三个行业，移动互联网、数据服务、游戏，他们在这6个工具的’提及率’上比较接近，除了游戏行业对sas的提及率最低（毕竟sas在金融和医药等行业用的更多，游戏行业感觉和他们’不是一路人’）这3个行业对各个工具感觉是雨露均沾，没有像前三个行业一样对某个工具比其他工具提及的更显著频繁&emsp;&emsp;总体来看，在常用分析工具里，spss可能是用的较少的一个 &emsp;&emsp;除了这6个工具，这6各个行业都有一些自己’专属’的工具：&emsp;&emsp;游戏行业对linux的需求比其他行业要高很多&emsp;&emsp;数据服务对java的需求也比其他行业高很多&emsp;&emsp;数据服务和企业服务对ppt的’提及率’仅次于这6个工具，充分体现了’服务’行业的特色 结尾&emsp;&emsp;&emsp;&emsp;以上分析了一些岗位信息以及它们和薪资之间的关系，相信可以给对不了解数据分析工作的同学提供一个直观的初步认识，做好这些工作之后，我也打算开始找自己的工作了，希望其他和我一样转行的同学们都能找到一个好的起点]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>拉勾网</tag>
        <tag>数据分析岗</tag>
        <tag>echarts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[房价预测(浓缩版)]]></title>
    <url>%2F2018%2F11%2F08%2Fhouse-price-sum%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;这个项目是kaggle上的入门项目之一，目标就是根据房屋信息来预测房价，这也是我做的第一个项目，因为能力有限目前到7%的位置就止步不前了。&emsp;&emsp;这个项目说难不难，不是多高深的项目，但想进前5%我都觉得有点困难。最主要的一点问题就是，我在没有删训练集样本的情况下，在训练集交叉验证的结果越差，提交上去的结果反而更好，我尝试加强正则化约束但结果还是变差，让我感觉除了多堆一些模型以外可能还是需要对特征做更多处理上的尝试吧。&emsp;&emsp;接下来，我先把我的baseline大致内容展示一下，具体操作可以看后面的博客。 了解数据&emsp;&emsp;先来认识一下数据:&emsp;&emsp;数据来源是美国Ames市2006-2010年的房屋信息，当初是用来作为数据科学教育来使用的。&emsp;&emsp;提供给我们的房屋信息有79个特征，我把它们粗略分为几个大类：周围环境、房屋结构、各结构的数量质量、其它信息（如风格、售卖时间等）。&emsp;&emsp;到这里我们还分辨不出明显没用的特征，但还是可以发现里面的变量有很多组合的机会，在后面会提到。&emsp;&emsp;除此之外，数据量很小，训练集和验证集总和不超过3000，但特征有79个，是一个很容易过拟合的数据&emsp;&emsp;现在回到目标，这里预测房价需要做回归分析,评估指标为rmse，适合做线性回归，所以我初步打算使用ridge、lasso和xgboost 数据清洗&emsp;&emsp;处理异常值，通过describe数据和画变量间散点图可以观察到3个’与众不同’的值&emsp;&emsp;提供给我们的数据里有很多变量(34个)包含缺失值，其中前四个变量绝大部分样本都缺失了，需要后面仔细研究一下,因为一般来说可能这样的变量可能要考虑删除。&emsp;&emsp;下面对这些问题具体分析。 异常值&emsp;&emsp;前两个异常值我们通过看图就可以发现，有两个样本居住面积分别为第一第二但售价较低，虽然不一定就是有误的数据，但样本量小且离群较严重，对模型的建立影响也较大，考虑删除1train.drop(train[(train['GrLivArea']&gt;4500)].index, inplace=True) &emsp;&emsp;还有一个样本车库建成年份填的2207年，而它的房子是2006年建的，并且2007年remodel过，应该是填错了，所以将它改过来1test['GarageYrBlt'][2593] = 2007 缺失值&emsp;&emsp;缺失值的处理占预处理的绝大部分，因为可以分成很多情况。为了简单展示，我只给每种情况列出一个例子，具体的可以看后面的博客。&emsp;&emsp;前面说到的有些变量绝大部分样本都有缺失，其实不都是缺失，原因在于这个数据，它有的变量可以填’NA’，可以在data_description里查看，但它把缺失值也录成’NA’，所以产生了混淆&emsp;&emsp;考虑到使用的ridge和lasso不能对缺失值处理，并且根据一些原则填补缺失值也能尽可能还原数据原貌，所以下面将展开对缺失值的处理。&emsp;&emsp;根据我对数据的观察，缺失值的处理分四类：&emsp;&emsp;1、根据官方的吩咐，Functional如果为NA就默认为’typical’1all_data['Functional'].fillna('Typ', inplace=1) &emsp;&emsp;2、值域不包含’NA’的变量，说明这些确实是缺失值，那就用一些方法填补，例如根据其他相关变量推测、填充中位数、众数等，例如下面：1all_data['Utilities'].fillna('AllPub', inplace=1) &emsp;&emsp;Utilities里除了一个样本其它都填的allpub，并且Utilities缺失的样本供暖用的GasA，电力系统FuseA，厨房质量中等最起码说明要有供水和下水道，所以用allpub填充。&emsp;&emsp;3、可以填’NA’的变量，因为真假缺失值之间产生混淆，我只能把可以分辨出是真·缺失值的找出来然后和上一步类似的方法进行填充，否则只能认为它们就是变量值为’NA’。&emsp;&emsp;例如PoolQC的缺失样本里，有三个填了PoolArea，说明有游泳池，我们就根据它们的房屋的总体质量来推测泳池质量123all_data.loc[2421, 'PoolQC'] = 'Fa'all_data.loc[2504, 'PoolQC'] = 'Gd'all_data.loc[561, 'PoolQC'] = 'Fa' &emsp;&emsp;4、剩下的都是上一步剩下的没辙的缺失值，统一地数值型填充为0(认为没有这一类的属性)，类别型填充为’None’&emsp;&emsp;处理之后： 新建变量&emsp;&emsp;在数据分析里，一个好的模型总是少不了一些高效的’人造’变量，根据业务理解对一些变量进行变换和组合往往能得到预测力更强的变量，很多时候模型的套用方法差别不大，拉开差距的是特征工程部分。&emsp;&emsp;其实在房屋结构和它们对应的数量质量里有很多可以造出的新变量，例如数据里有关于关于房屋各个部分的面积，我们很难说住房第一层的面积大的售价就高，但如果总体面积大，例如有两层房并且有宽敞或者多个车库，那么通常来说房子会更之前一些；或者说一个房子是否有两层，通常代表两种设计，关于国外的房价不是很清楚，但在这个数据集里单层要比多层的贵一些。通过类似的方式可以造出很多变量，我只取里面效果较好并且比较有解释力的4个组合：&emsp;&emsp;房屋总建成面积、所有走廊总面积、是否有第二层、是否有木质地板12345678all_data['total_fin_sf'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])all_data['total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])all_data['has2ndfloor'] = (all_data['2ndFlrSF']==0)*1all_data['WoodDeckSF'] = (all_data['WoodDeckSF']==0)*1 特征工程的尾声&emsp;&emsp;前面已经对变量进行了清洗和变换，接下来就是要建立模型了，但在这之前，我们还需要对数据进行一些处理，让它们能更适合模型学习&emsp;&emsp;对连续型数据我们通常需要做scaling，它能保障损失函数收敛的速度，否则在某些变量上已经收敛但其它一些变量还有很长一段路要走。我在后面套用模型时直接使用pipline加上robust_scale，在做scaling的基础上，给数据增加了一定的鲁棒性。&emsp;&emsp;除此之外还有3步要做： 数据类型转换&emsp;&emsp;MSSubClass虽然是数字类型，其实每个数字代表类别，并不是连续关系，所以需要把他们变成类别变量&emsp;&emsp;同理，Mosold是指售出月份，我觉得也不存在连续关系，所以做一样的处理12all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)all_data['MoSold'] = all_data['MoSold'].apply(str) 偏度处理&emsp;&emsp;线性回归的假设前提就是要满足正态分布，偏度过大的数据会影响拟合效果，在这之前需要对偏度进行一下处理。&emsp;&emsp;我这里对y和偏度大于2的x的连续变量值取log(1+x)，处理偏度的方法还有coxbox，两者在这里的效果接近。123456loged_features = all_data.loc[:, all_data.columns[all_data.dtypes!='object']].apply(skew)\.sort_values(ascending=False)[:12].indexall_data[loged_features] = all_data[loged_features].applymap(np.log1p)y_train = np.log1p(y_train) one-hot&emsp;&emsp;除了xgb以外，lasso和ridge根本不能对类别变量进行处理，所以需要对类别变量进行one-hot处理1all_data = pd.get_dummies(all_data) 还有一些没做的&emsp;&emsp;数据的变量数量相对于样本数有一些多，尤其是通过one-hot后变量数更多，可以筛掉一部分的变量来减轻过拟合的风险。本来我打算做个feature_selection，但不管怎么删除，我做交叉验证结果有提升，但提交上去成绩反而下降，所以暂时拿这个没办法。。。就只能保持原样了&emsp;&emsp;到这里特征处理基本已经完成，下面就开始进行建模了 建模&emsp;&emsp;建模部分和套模板一样，对于ridge和lasso来说只要通过cv找到合适的正则化系数就好，而xgb的参数多一些需要做GridSearch。&emsp;&emsp;本来我用lasso和xgb就是看中lasso的l1正则化和xgb的sampling、feature_combination和l1、l2正则化，来降低过拟合。除此之外，我使用mlxtend里的StackingCVRegressor对上面三个模型做一个stacking，进一步降低过拟合，它给我的成绩带来了不小的提升 Ridge1ridge = make_pipeline(RobustScaler(), Ridge(alpha=15.2)) lasso1lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0004, max_iter=1e7)) xgboost123456789101112131415gsearch = GridSearchCV(estimator = XGBRegressor(), param_grid=&#123; 'max_depth':[3,4,5], 'subsample':[0.5,0.6,0.7], 'colsample_bytree':[0.5,0.6,0.7], 'min_child_weight':[0,1,2], 'gamma':[0,0.003,0.005], 'seed':list(range(20)) &#125;, scoring='neg_mean_squared_error', cv=5)xgb = make_pipeline(RobustScaler(), XGBRegressor(learning_rate =0.01, n_estimators=3500, max_depth=3, min_child_weight=0 , gamma=0, subsample=0.6, colsample_bytree=0.6, reg_alpha=0, seed=17)) stacking1stack = StackingCVRegressor(regressors=(ridge, lasso, xgb), meta_regressor=xgb, use_features_in_secondary=True) 预测房价12345678910111213ridge.fit(x_train, y_train)y_ridge = np.expm1(ridge.predict(x_test))lasso.fit(x_train, y_train)y_lasso = np.expm1(lasso.predict(x_test))xgb.fit(x_train, y_train)y_xgb = np.expm1(xgb.predict(x_test))stack.fit(x_train, y_train)y_stack = np.expm1(stack.predict(x_test))predict = 0.5*y_stack + 0.1*y_ridge + 0.2*y_lasso + 0.2*y_xgb 总结&emsp;&emsp;这个项目主要有这些关键点 缺失值较多，并且有些并不是真的缺失，所以处理起来要细心一些 根据业务新建一些好的变量可以达到更好的学习效果，可以进行更多的尝试 样本量有限的数据，在确定算法后，可以尝试用一些去过拟合的方法，例如使用robust_scale，多模型取长补短（stacking）。]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>kaggle入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[房价预测 —— 预处理]]></title>
    <url>%2F2018%2F11%2F02%2Fhouse-price1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在前面我们已经把项目步骤过了一遍，下面的这些是更具体的处理，从预处理到建立模型。 大致观察&emsp;&emsp;首先我们来看看数据基本情况，看看分布，做做连续变量间相关，哪些变量有缺失值，图就不放了，同学们想象一下（其实是那个图太大了。。，其他结果直接在后面的处理里会出现）。脑补之后，我们发现了一些问题：1.一个异常年份&emsp;&emsp;一个人在车库建成年份填了2207年，有屁股的人都会觉得他可能想写2007年，去看一下相关变量，他的房子是2006年建的并且2007年得到了’强化’，没得说，就把它改成20071test['GarageYrBlt'][2593] = 2007 2.两个肉眼看上去不科学的样本&emsp;&emsp;右下角的两个点，面积在所有的里面最大，房价却算得上廉价了，最主要是他们两个偏离大众有点远，不管你是真实还是乱填的数据，都得删掉这两个毒瘤1train.drop(train[(train['GrLivArea']&gt;4500)].index, inplace=True) 3.很多缺失值&emsp;&emsp;大概有34个变量存在缺失值，就不列出来了，后面处理时自然能看到有哪些。在处理之前我想说一点，这个数据集有一个坑，很多变量里就设有NA这个变量值，用来表示没有这类东西，但是他们把缺失值也录成NA，所以在包含NA变量值的变量下，真的值是NA还是缺失了，傻傻分不清。。。我们只能根据其他变量推测这个可能确实是缺失值，没有其他信息就只能当作没缺失。下面很长一部分都是在处理缺失值： 处理缺失值 Functional&emsp;&emsp;这个变量，在数据描述里说了，如果没有就默认是typical，所以就直接这样修改吧1all_data['Functional'].fillna('Typ', inplace=1) 值域不包含NA的变量&emsp;&emsp;这些变量分别是： MasYnrType MSZoning Utilities Exterior2nd Exterior1st SaleType Electrical KitchenQual MasYnrType&emsp;&emsp;有一个样本masvnrarea值为189但masynrtype为nan，说明这个应该是缺失值，我使用’BrkFace’来填充，因为在各种单板墙里它占绝大多数且类型为brkface的单板强面积中位数也比较接近1891234567891011121314all_data['MasVnrType'].value_counts() -&gt; None 1742 BrkFace 879 Stone 247 BrkCmn 25 Name: MasVnrType, dtype: int64all_data.groupby('MasVnrType')['MasVnrArea'].median() -&gt; MasVnrType BrkCmn 161.0 BrkFace 203.0 None 0.0 Stone 200.0 Name: MasVnrArea, dtype: float64 &emsp;&emsp;其它23个nan值没看出来什么端倪，但是我想搞点事情，我觉得有单板墙比没有的可能要高档一点吧，确实没有单板墙的房子OverallQual要低一截，并且这23个房子的OverallQual在平均之上，但我在尝试用有单板墙的房子的type的众数和area平均数来填补，还不如把他们当作没有单板墙来处理（因为确实两个值都没有很有可能就是没有单板墙）123all_data['MasVnrType'].loc[2611]='BrkFace'all_data['MasVnrType'].fillna('None', inplace=1)all_data['MasVnrArea'].fillna(0, inplace=1) MSZoning&emsp;&emsp;这个我就直接用众数填充了1all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0], inplace=1) Utilities非缺失值里除了一个样本是NoSeWa其他都是allpub，并且这两个缺失了的样本供暖都是用的GasA，电力系统是FuseA，厨房质量中等，用水和下水道肯定要有吧，所以他们就是allpub1all_data['Utilities'].fillna('AllPub', inplace=1) exterior1st, exterior2nd&emsp;&emsp;就一个样本同时缺失了这两个值，首先可以肯定他们有涂料，因为ExterQual为TA，至于有没有第二种涂料就不知道了，都给他们用众数替代吧，反正绝大部分房子都没有第二种涂料12all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0], inplace=1)all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0], inplace=1) saletype, electircal&emsp;&emsp;这俩货就都用众数填充12all_data['SaleType'].fillna(all_data['SaleType'].mode()[0], inplace=1)all_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=1) kitchenqual&emsp;&emsp;厨房质量缺失的就一个样本，这个房子总体质量为5,那就用TA来填充吧1all_data['KitchenQual'].fillna('TA', inplace=1) 可以填NA的变量poolqc&emsp;&emsp;有3个样本poolqc没填但是填了poolarea，说明还是有游泳池，那就根据房屋总体质量太填充游泳池的质量，其他样本不清楚就替换成’None’123all_data.loc[2421, 'PoolQC'] = 'Fa'all_data.loc[2504, 'PoolQC'] = 'Gd'all_data.loc[561, 'PoolQC'] = 'Fa' miscfeature&emsp;&emsp;有一个样本miscfeature没填但是miscval为17000,这个还是所有miscval里最高的，而在所有miscfeature里就车库的价值最高，并且我们可以看到他的车库质量和现状处于平均水平，但是有3个车位，所以可能他有第二个车库（当然也可能是我想多了），其它nan的就用’None’替换1234567all_data[['GarageCars', 'GarageQual', 'GarageCond']].loc[2550] -&gt; GarageCars 3 GarageQual TA GarageCond TA Name: 2550, dtype: objectall_data['MiscFeature'][2550] = 'Gar2' garage系列&emsp;&emsp;和车库有关的存在缺失值的变量里，有两个不是完全缺失，他们的车库类型填的Detchd，也就是说应该有车库，那这两个样本的其他缺失值可以根据车库类型为Detchd的样本总体情况来填充12345678910all_data['GarageArea'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median()all_data['GarageCars'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageCond'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageCond'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageFinish'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageFinish'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageQual'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageQual'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageYrBlt'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median()all_data['GarageYrBlt'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median() &emsp;&emsp;剩下garageyrblt为nan的样本都是没有车库的，就把他们都作为一类，赋值为0，本来我不是这样做的，但是分配跟它们一个年份反而结果更差basement系列&emsp;&emsp;在和地下室有关的变量里，如果一个样本缺失值超过1个，那这个样本所有的值不是0就是nan，所以可以把他们都当作没有地下室。只有一个缺失值的样本都是类比变量，可以根据其它变量来填充&emsp;&emsp;例如，BsmtFinType2根据BsmtFinSF2估计为ALQ、3个BsmtExposure缺失的地下室都还没建好，就按照绝大多数情况定为’No’、BsmtCond都用TA、BsmtQual缺失的两个值，他们的cond分别是Fa和TA，并且都光线不好所以给Fa和Po123456789all_data.loc[333, 'BsmtFinType2'] = 'ALQ'all_data.loc[949, 'BsmtExposure'] = 'No'all_data.loc[1488, 'BsmtExposure'] = 'No'all_data.loc[2041, 'BsmtCond'] = 'TA'all_data.loc[2186, 'BsmtCond'] = 'TA'all_data.loc[2218, 'BsmtQual'] = 'Fa'all_data.loc[2219, 'BsmtQual'] = 'Po'all_data.loc[2349, 'BsmtExposure'] = 'No'all_data.loc[2525, 'BsmtCond'] = 'TA' lotfrontage&emsp;&emsp;房子到街道的距离，要是在天朝像我们大部分人怕不是都只有几米，只有住在比较闲适或偏僻的地方或距离远一些，国外这个情况可能还更明显，所以可以试着根据neighborhood来填充12all_data['LotFrontage'] = \all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median())) 其它的缺失值&emsp;&emsp;最后剩下的都是暂时没看出个门道来的，所以类型变量全填充为’None’，数值型的全填充为012345all_data.loc[:, all_data.columns[all_data.dtypes=='object']] = \all_data.loc[:, all_data.columns[all_data.dtypes=='object']].fillna('None')all_data[['BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'TotalBsmtSF']] = \all_data[['BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'TotalBsmtSF']].fillna(0) 新建变量&emsp;&emsp;新增4个变量，用来描述总完成的面积、所有类型走廊总面积、是否有第二层和是否有铺置木质地板12345678all_data['total_fin_sf'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])all_data['total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])all_data['has2ndfloor'] = (all_data['2ndFlrSF']==0)*1all_data['WoodDeckSF'] = (all_data['WoodDeckSF']==0)*1 数据类型转换&emsp;&emsp;MSSubClass虽然是数值类型，但实际上各个值并不是连续的关系，所以还是转换成类别数据;变量Mosold是指售出的月份，12个月其实不存在先后顺序吧，我觉得应该变成类别数据12all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)all_data['MoSold'] = all_data['MoSold'].apply(str) 偏度处理&emsp;&emsp;对偏度大于2的特征取log(1+x)，对目标变量也是一样123456loged_features = all_data.loc[:, all_data.columns[all_data.dtypes!='object']].apply(skew)\.sort_values(ascending=False)[:12].indexall_data[loged_features] = all_data[loged_features].applymap(np.log1p)y_train = np.log1p(y_train) one-hot&emsp;&emsp;将类别变量做one-hot处理1all_data = pd.get_dummies(all_data) &emsp;&emsp;之后我没做feature selection，删少了没效果，多删一点我做cv分数有提高，但是提交上去更差。到这里就基本上对数据预处理完了，接下来主要就是套模型，找到和合适的参数，然后再做一个stacking]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>kaggle入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[房价预测 —— 建模]]></title>
    <url>%2F2018%2F11%2F02%2Fhouse-price2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;这里我使用了ridge、lasso、xgboost，然后对他们进行stacking，最后把这四个的预测值加权求平均值12345678from sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lassofrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import GridSearchCVfrom xgboost import XGBRegressorfrom mlxtend.regressor import StackingCVRegressor ridge&emsp;&emsp;先手动找一下ridge比较好的l2值，这里是先粗略看下分布，后面在15附近又搜索了一遍，确定alpha=15.21234567891011def ridge_test(alpha): reg = make_pipeline(RobustScaler(), Ridge(alpha=alpha)) score = np.sqrt(-cross_val_score(reg, x_train, y_train, scoring='neg_mean_squared_error', cv=10)).mean() return scoreridge_scores = []alphas = list(range(1,30))+[40,50,60]for alpha in alphas: ridge_scores.append(ridge_test(alpha))plt.plot(alphas, ridge_scores) &emsp;&emsp;可以看到在15附近rmse值最低，最后确定为15.21ridge = make_pipeline(RobustScaler(), Ridge(alpha=15.2)) #lasso&emsp;&emsp;用类似上面的方法找lasso模型比较合适的正则化系数1234567891011lasso_scores = []def lasso_test(alpha): lasso = make_pipeline(RobustScaler(), Lasso(alpha=alpha, max_iter=1e7)) lasso_scores.append(\ np.sqrt(-cross_val_score(lasso, x_train, y_train, cv=5, scoring='neg_mean_squared_error')).mean())alphas = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]for alpha in alphas: lasso_test(alpha)plt.plot(alphas, lasso_scores)plt.show() &emsp;&emsp;最后选择alpha=0.00041lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0004, max_iter=1e7)) xgboost&emsp;&emsp;xgb需要用gridsearch来挑选参数，我是根据google到的一个顺序来挑选参数，但我感觉这个方法不够’优秀’，因为每次只对一到两个参数做gridsearch，会错过很多各个参数的搭配，但是一次调试的参数多起来，电脑吃不消。不知道是有更多调试的技巧，还是高手都是用服务器或者还有其他加速方式以供他们搜索大量的情况，以后有机会要请教一下&emsp;&emsp;这是后来缩小范围的gridsearch：1234567891011gsearch = GridSearchCV(estimator = XGBRegressor(), param_grid=&#123; 'max_depth':[3,4,5], 'subsample':[0.5,0.6,0.7], 'colsample_bytree':[0.5,0.6,0.7], 'min_child_weight':[0,1,2], 'gamma':[0,0.003,0.005], 'seed':list(range(20)) &#125;, scoring='neg_mean_squared_error', cv=5) &emsp;&emsp;最后：123xgb = make_pipeline(RobustScaler(), XGBRegressor(learning_rate =0.01, n_estimators=3500, max_depth=3, min_child_weight=0 , gamma=0, subsample=0.6, colsample_bytree=0.6, reg_alpha=0, seed=17)) stacking&emsp;&emsp;通过对上面3个模型的预测值再进行学习，得到新的模型。同时这个方法有个参数use_features_in_secondary，如果选中，还会将原始数据放进来学习，这里我选True，元分类器我选择之前的xgb1stack = StackingCVRegressor(regressors=(ridge, lasso, xgb), meta_regressor=xgb, use_features_in_secondary=True) 汇总结果&emsp;&emsp;最后用这4个模型对测试集进行预测，并将结果加权平均（记得把y用np.expm1转化回来）12345678910111213ridge.fit(x_train, y_train)y_ridge = np.expm1(ridge.predict(x_test))lasso.fit(x_train, y_train)y_lasso = np.expm1(lasso.predict(x_test))xgb.fit(x_train, y_train)y_xgb = np.expm1(xgb.predict(x_test))stack.fit(x_train, y_train)y_stack = np.expm1(stack.predict(x_test))predict = 0.5*y_stack + 0.1*y_ridge + 0.2*y_lasso + 0.2*y_xgb &emsp;&emsp;到这里，一个baseline就算完成了，剩下的就是根据理解和猜想去调整特征和模型，我做到前8%就放弃了，确实是黔驴技穷。不过说回来，入门项目虽然简单，但是能很好地让新手认识机器学习基本的流程，随着以后掌握的技能越来越多，就可以在原来的基础上提高或接触更有挑战性的项目。kaggle上的讨论和分享要比国内的这几个竞赛网站好很多，这一点也是最吸引我的地方，就看以后有没有机会回来给这个小项目来一个质的提升吧。]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>kaggle入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用评分卡 —— 预处理]]></title>
    <url>%2F2018%2F11%2F01%2Fcredit-card1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;数据来自于kaggle的一个信用评估项目，比赛早就停止了，我就拿它的数据来做了一个简单的评分卡，话不多说，先来看看数据 数据预览&emsp;&emsp;数据的特征很少，总共10个自变量，下面是关于自变量的定义： 特征名 描述 RevolvingUtilizationOfUnsecuredLines Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits age Age of borrower in years NumberOfTime30-59DaysPastDueNotWorse Number of times borrower has been 30-59 days past due but no worse in the last 2 years. DebtRatio Monthly debt payments, alimony,living costs divided by monthy gross income MonthlyIncome Monthly income NumberOfOpenCreditLinesAndLoans Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards) NumberOfTimes90DaysLate Number of times borrower has been 90 days or more past due. NumberRealEstateLoansOrLines Number of mortgage and real estate loans including home equity lines of credit NumberOfTime60-89DaysPastDueNotWorse Number of times borrower has been 60-89 days past due but no worse in the last 2 years. NumberOfDependents Number of dependents in family excluding themselves (spouse, children etc.) &emsp;&emsp;目标特征则是SeriousDlqin2yrs，用0或1表示是否坏用户&emsp;&emsp;数据分布就不在博客里展示了，太难看，同学们可以自己自己画一下&emsp;&emsp;下面是各个特征相关系数图： 大致思路&emsp;&emsp;通过前面对数据的观察，我发现了几个问题，并采用以下的处理办法：1、age=0&emsp;&emsp;训练集里有一个样本age为0，属于乱填，并且这个情况在测试集里没有出现&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;删除该样本1train.drop(train[train['age']==0].index, inplace=True) 2、特征冗余&emsp;&emsp;这里的老赖分3个等级，拖了30-59天、60-89天、超过90天的，这3个变量相关度很高，说明基本都是一批人&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;保留和目标特征相关度较高的一个12train.drop(['60-89DaysPast', '90DaysLate'], axis=1, inplace=True)test.drop(['60-89DaysPast', '90DaysLate'], axis=1, inplace=True) 3、debtratio&gt;1&emsp;&emsp;这个特征是指还款、赡养费、生活开销占毛月收入比例，训练集有35137个样本大于1（测试集也不少）&emsp;&emsp;对于这个我有几个猜想：&emsp;&emsp;&emsp;&emsp;1.确实入不敷出，可能收入差的一点钱家里一起分担掉了，那么dabtratio应该接近1的&emsp;&emsp;&emsp;&emsp;2.家里有矿、有人包养，月收入几乎没有但挺会花钱的&emsp;&emsp;&emsp;&emsp;3.debtratio可能是官方根据支出和收入计算来的，而有的人收入可能乱填成很小&emsp;&emsp;&emsp;&emsp;4.收入没填的，可能系统默认当作1，因为分母不能为0（在原数据里可以看到，收入缺失的debtratio都是几位数）&emsp;&emsp;&emsp;&emsp;5.可能这个人很多信息都是乱填的&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;不管这些有的没的猜想，反正是不好直接处理，只能通过后面分箱，看能不能把大于1的样本分出几部分，如果不能就作为一类。&emsp;&emsp;另外：考虑到debtratio数据可能是后期算来的，如果debt统计正确的话，我们可能可以把它做成一个新变量12345pos_train_Income = resample_train['MIncome'].replace([0, -10000], 1)resample_train['Debt'] = pos_train_Income * resample_train['DebtRatio']pos_test_Income = test['MIncome'].replace([0, -10000], 1)test['Debt'] = pos_test_Income * test['DebtRatio'] 4、MonthlyIncome的缺失和异常值&emsp;&emsp;训练集有29731个缺失值（测试集也很多），缺失值太多，而该特征和其他特征相关度都极低，用机器学习补全也效果不好，同时也有很多很低的值，例如个位数、两位数等，我们不能否认可能部分极低值的真实性&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;月收入缺失的样本单独作为一类&emsp;&emsp;&emsp;&emsp;收入极低的样本通过后期分箱看能分出几组来12train['MIncome'].fillna(-10000, inplace=True)test['MIncome'].fillna(-10000, inplace=True) 5、RevolvingUtilizationOfUnsecuredLines&gt;1&emsp;&emsp;这个变量指信用额度的使用率，超过1肯定是有问题，有可能和debtratio一样，是因为额度统计错误等原因&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;后期分箱看能否划分出几个组6、dependents缺失&emsp;&emsp;训练集里有3924个样本在dependents上缺失，缺失量不太大，并且和age、NumberRealEstateLoansOrLines有一定相关&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;机器学习来补齐数据，这里选择randomforest1234567891011121314rf_train_x = train[train['MIncome']!=-10000].iloc[:, [1,2,3,4,6,7]]rf_train_y = train[train['MIncome']!=-10000]['Dependents']# 不想用不填收入的人，因为所有没填dependents的人都没填收入，他们虽然可能是一类人，但是他们的数据可能全都是乱写的from sklearn.ensemble import RandomForestRegressorclf = RandomForestRegressor(random_state=0, n_estimators=500, n_jobs=-1)clf.fit(rf_test_x, rf_test_y)clf.score(rf_test_x, rf_test_y)# -&gt; 0.87447288413972102# 就不做交叉验证了dep_train = np.round(clf.predict(train[train['Dependents'].isnull()].iloc[:, [1,2,3,4,6,7]]))dep_test = np.round(clf.predict(test[test['Dependents'].isnull()].iloc[:, [1,2,3,4,6,7]]))test['Dependents'][test['Dependents'].isnull()] = dep_test 7、NumberOfOpenCreditLinesAndLoans和NumberRealEstateLoansOrLines&emsp;&emsp;前者指信用贷款和抵押贷款类的贷款数量，后者指抵押贷款和不动产借贷数量&emsp;&emsp;感觉前者部分包含了后者，并且所有样本在前一个特征的值都比后一个大(让我觉得不是巧合)&emsp;&emsp;考虑建立一个新变量12345resample_train['credit'] = resample_train['OpenL2'] - resample_train['REL']test['credit'] = test['OpenL2'] - test['REL']resample_train.to_csv('train_fixed1.csv', index=False)test.to_csv('test_fixed1.csv', index=False) 8、数据不平衡&emsp;&emsp;正负样本比例约为14:1&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;采用smote升采样至10:1，然后多次对正样本采样生成几组训练集，对学习结果求均值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 代码是从简书上借鉴来的，用的是k临近值求均值或众数充当新数据# 这个函数有个小地方要注意# 如果一个变量unqiue值数量小于kdistinctvalue，会被认为是类别变量，会对k临近值取众数# 如果设置的较小，可能会取到k个点都不一样，这样会返回nandef smote(data, tag_index=None, max_amount=0, std_rate=5, kneighbor=5, kdistinctvalue=10, method='mean'): try: data = pd.DataFrame(data) except: raise ValueError case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count() case_rate = max(case_state) / min(case_state) location = [] if case_rate &lt; 5: print('不需要smote过程') return data else: less_data = np.array( data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[0]]) more_data = np.array( data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[0]]) neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data) for i in range(len(less_data)): point = less_data[i, :] location_set = neighbors.kneighbors([less_data[i]], return_distance=False)[0] location.append(location_set) if max_amount &gt; 0: amount = max_amount else: amount = int(max(case_state) / std_rate) times = 0 continue_index = [] class_index = [] for i in range(less_data.shape[1]): if len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue: continue_index.append(i) else: class_index.append(i) case_update = list() location_transform = np.array(location) while times &lt; amount: new_case = [] pool = np.random.permutation(len(location))[1] neighbor_group = location_transform[pool] if method == 'mean': new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=0) if method == 'random': away_index = np.random.permutation(len(neighbor_group) - 1)[1] neighbor_group_removeorigin = neighbor_group[1:][away_index] new_case1 = less_data[pool][continue_index] + np.random.rand() * ( less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index]) new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[0, :]) new_case = list(new_case1) + list(new_case2) if times == 0: case_update = new_case else: case_update = np.c_[case_update, new_case] print('已经生成了%s条新数据，完成百分之%.2f' % (times, times * 100 / amount)) times = times + 1 less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index])) more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index])) data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T))) label_columns = [0] * more_origin_data.shape[0] + [1] * ( less_origin_data.shape[0] + np.array(case_update.T).shape[0]) data_res = pd.DataFrame(data_res) return data_resresample_train = smote(train, tag_index=0, std_rate=10, kdistinctvalue=8) resample_train[['age', '30-59DaysPastDue', 'MIncome', 'OpenL2', 'REL', 'Dependents']] = \resample_train[['age', '30-59DaysPastDue', 'MIncome', 'OpenL2', 'REL', 'Dependents']].applymap(round)resample_train['MIncome'].replace(-6000, -10000, inplace=True) #用smote造出的收入有-10000和-6000，依然改为-10000 &emsp;&emsp;一个简单的数据预处理就做完了，接下来就是对变量进行卡方分箱]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用评分卡 —— 卡方分箱和变量筛选]]></title>
    <url>%2F2018%2F11%2F01%2Fcredit-card2%2F</url>
    <content type="text"><![CDATA[预备工作&emsp;&emsp;首先定义一个分箱函数。我这里是采用合并的方式进行分箱，对相邻的组进行卡方检验，取里面结果最差的，如果显著，我这里设置的p值是0.01，则保留这两组，结束分箱，否则这两组合并。12345678910111213141516171819202122232425262728293031def chi2(freq, min_e=0.5): N = freq.sum().sum() r = freq.sum(axis=1) c = freq.sum(axis=0) chis = 0 for i in range(len(freq)): for j in range(len(freq.columns)): o = freq.iloc[i, j] e = r.iloc[i]*c.iloc[j]/N e = e if e &gt;= min_e else min_e chis += ((o-e)**2) / e return chisdef merge(freq, min_score=9.21): n = len(freq)-1 while True: scores = &#123;&#125; for i in range(n): score = chi2(freq.iloc[(i, i+1), :]) scores.setdefault(score, []).append((i, i+1)) floor = min(scores.keys()) if floor &lt; min_score: for left, right in list(reversed(scores[floor])): freq.iloc[left] += freq.iloc[right] freq.drop(freq.index[right], axis=0, inplace=True) n -= 1 else: break print(freq.index) print(Series(scores).sort_values()) &emsp;&emsp;因为是连续变量，而且unique值太多，从原始数据两两做卡方检验能做茫茫多年，所以我对数据先分一些组，例如比例型的值域0-1,就把他们保留两位小数，这样unique值不会超过100个(强迫症会说是101个)，对结果影响很小，但可以大大提高速度1234567891011121314151617def rd(x): if x&lt;=1: return round(x,2) elif x&lt;=10: return int(x)+1 elif x&lt;=100: return int(x/5+1)*5 elif x&lt;=1000: return int(x/100+1)*100 elif x&lt;=10000: return int(x/1000+1)*1000 elif x&lt;=100000: return int(x/10000+1)*10000 elif x&lt;=1000000: return int(x/100000+1)*100000 else: return int(x/1000000+1)*1000000 &emsp;&emsp;因为最后分箱时需要用到每个箱之间的边界，所以我就把两组间的边界定义成前一组的最大值和后一组最小值的平均数1234def get_border(name, groups): min_max = train[name].groupby(train[name].map(rd)).agg(['min', 'max']) trans = Series((min_max['min'].iloc[1:].values+min_max['max'].iloc[:-1].values)/2, index = min_max.index[1:]) return trans[groups] &emsp;&emsp;分箱后我们需要根据woe值和iv值判断变量的好坏，并且woe值将替换原数据作为新的x，所以我们也把计算woe和iv值的函数定义一下(在这个函数里我直接根据分箱把原数据替换成woe值)12345678910111213141516171819202122232425def good(x): return (len(x)-sum(x))/good_sumdef bad(x): return sum(x)/bad_sumbad_sum = int(train['target'].sum())good_sum = len(train['target']) - bad_sumivs = &#123;&#125;def woe_iv(var, cut): data = Series(train['target'].values, train[var].values) data.index = pd.cut(data.index, cut, right=False) print(data.groupby(data.index).count()) ratio = data.groupby(data.index).agg([good,bad]) woe = np.log(ratio['good']/ratio['bad']) sub = ratio['good']-ratio['bad'] iv = (sub*woe).sum() ivs[var] = iv print('woe:') print(woe, '\n') print('iv:', iv) train[var] = pd.cut(train[var], cut, right=False).map(woe.to_dict()) test[var] = pd.cut(test[var], cut, right=False).map(woe.to_dict()) print(woe.to_dict()) 开始分箱&emsp;&emsp;接下来大致套用上面的函数就可以了，但是可以根据情况进一步合并，例如有的组全是好样本但是数据量很小，可以考虑和前一组合并&emsp;&emsp;下面展示对age变量的分箱：12345678910111213141516171819202122232425262728293031323334353637freq_age = pd.crosstab(train.age, train.target)merge(freq_age)-&gt; Int64Index([21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99], dtype='int64', name='age') 21.763776 [(0, 1)] 32.122824 [(1, 2)] 12.092637 [(2, 3)] 24.292766 [(3, 4)] 23.000035 [(4, 5)] 34.589954 [(5, 6)] 14.700949 [(6, 7)] 11.499217 [(7, 8)] 133.779114 [(8, 9)] 18.759893 [(9, 10)] 58.973590 [(10, 11)] 9.492844 [(11, 12)] 12.886260 [(12, 13)] dtype: objectwoe_iv('age', [21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99, 1000])-&gt; woe: [21, 23) 0.536632 [23, 25) -0.172970 [25, 33) -0.569851 [33, 37) -0.462247 [37, 48) -0.328169 [48, 54) -0.227933 [54, 57) -0.048145 [57, 58) 0.166081 [58, 63) 0.352834 [63, 66) 0.897501 [66, 73) 1.137055 [73, 84) 1.627105 [84, 99) 2.043198 [99, 1000) 0.083389 dtype: float64 iv: 0.354380135746 &emsp;&emsp;可以看到分箱后iv值为0.35,具有一定的预测力，适合作为变量。同时最后一个分箱woe值较小并且样本数较少，可以考虑和前一组合并（不过我还是选择保留。。）&emsp;&emsp;其它的变量也是进行类似的处理，具体的可以直接看源码&emsp;&emsp;最后再通过xgboost来排除预测能力较差的变量123456from xgboost import XGBClassifierclf = XGBClassifier()clf.fit(train.iloc[:,[0,1,2,3,4,5,6,7,9,10]], train['target'])from xgboost import plot_importanceplot_importance(clf)plt.show() &emsp;&emsp;经过对所有变量进行以上的处理，结果如下：iv&gt;1 f_name iv RuOfUL 1.84 额度使用率越高失信率越高 30-59DaysPastDue 1.64 老赖和失信是扯不开的 &emsp;&emsp;虽然iv值大于1有过于美好的怀疑，但是我感觉这两个变量还阔以用0.3&lt;iv&lt;0.5 f_name iv age 0.354 从样本分布来看还是中年人更油腻一点呀 0.1&lt;0v&lt;0.3 f_name iv debtratio 0.18 0-0.4时debtratio越高失信率越高，但0.4-1变化就不明显了，大于1的就不解释了 REL(淘汰) 0.13 本身就iv值较低，xbgoost对它的评价也很低，就抛弃这个变量了 dependents 0.24 家庭成员越多，失信率越低 MIncome 0.09 2500左右工资的人失信率高一些，除此之外还是可以看到收入越高失信率越低，只是变化幅度较小，对于iv值介于0.05到0.1的变量，可以酌情考虑保留，我就很通情达理 &emsp;&emsp;还有一些变量因为iv值过低直接排除掉，很不幸，之前自作聪明造的两个变量都不是很好&emsp;&emsp;接下来要做的就是开始建模了，需要对不平衡进行第二次处理，使用LR，最后给用户进行评分]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信用评分卡 —— 建模与评分]]></title>
    <url>%2F2018%2F11%2F01%2Fcredit-card3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;下面将对数据套用LR模型，然后根据模型参数和woe值对样本进行评分，这个评分只是一个初步的评分，后续还有许多工作需要做来调整这个评分，不过我不是很了解这些，所以先就当一个初级工吧。&emsp;&emsp;首先我们留出30%的数据用以评估模型:12X_train, X_test, y_train, y_test = train_test_split( train.iloc[:,:-1], train['target'], test_size=0.3, random_state=42) 再次处理不平衡&emsp;&emsp;之前的处理我只把正负样本比例升到10:1，因为这个方法容易导致过拟合，但实际上10:1的比例还是有点高，而如果直接降采样对这个数据量不是很大的数据集还是会有很多信息损失&emsp;&emsp;所以，我将随机n次进行降采样，每次随机抽取一半的正样本和所有负样本合并作为新数据集进行学习，最后对这n个数据集学习的结果求均值得到最终的结果 下采样+拼接：1234567891011def undersample(X, y, subsets=7): data = X.join(y) train_sets = [] neg = data[data['target']==0] pos = data[data['target']==1] for i in range(subsets): re_neg = neg.sample(frac=0.5) combine = pd.concat([re_neg, pos]) train_sets.append(combine) return train_setstrain_sets = undersample(X_train, y_train, subsets=7) 定义训练函数123456789101112131415161718# LR的c=2是我之前对7组数据分别交叉验证找到的较合适的参数值，因为步骤多且写出来意义不大，接直接跳过def training(data_sets, test): test_results = [] coefs = [] intercepts = [] model = LogisticRegression(C=2) for data in data_sets: y_train = data['target'] x_train = data.iloc[:,:-1] clf = model.fit(x_train.values, y_train.values) coefs.append(clf.coef_) intercepts.append(clf.intercept_) test_results.append(clf.predict_proba(test)[:,1]) return &#123;'test_results':test_results, 'coefs':coefs, 'intercepts':intercepts&#125;# 这里画不了学习曲线，在开始采样时正样本比例过低，会报错 先用70%的数据学习123456test_result = training(train_sets, X_test)results = DataFrame(test_result['test_results']).mean()from sklearn.metrics import roc_curve, aucfpr, tpr, thresholds = roc_curve(y_test.values, results.values)-&gt; auc:0.88 roc曲线ks曲线&emsp;&emsp;这个结果应该还算可以吧，用另外30%且样本分布和原来不同的数据测试还有0.88的auc，ks也有0.61，泛化能力还算凑合 用上所有的数据接下来我们把所有的数据扔进模型，最终需要的就是所有的系数，也就是斜率coefs和截距intercept1234train_sets = undersample(train.iloc[:,:-1], train['target'], subsets=7)training_result = training(train_sets, test.iloc[:, :-1])intercept = DataFrame(training_result['intercepts']).mean()[0]coefs = DataFrame(np.array(training_result['coefs'])[:,0,:]).mean() 评分&emsp;&emsp;最后就是激动人心的评分环节了。我们当然可以对每个用户用LR模型返回是坏客户的概率并基于这个概率打分，但是也可以更简单，我们使用坏客户概率和好客户概率比例的ln值，也一样能体现该用户更像好客户和坏客户，实际上就是sigmoid的反函数，这样重新变回了一个线性问题&emsp;&emsp;我们只需要用数据值（也就是woe值）以及LR的系数就可以来预测用户信用，这样实施起来也很方便，当获得用户信息时根据其所在档位woe值和系数就可以知道他的预测信用水平，以0作为分界线&emsp;&emsp;当然我们不可能直接用个分数，我们平时看到的信用值都是几百的，所以要简单线性变换一下，我这里就用当好坏比例为20和10分别对应600分和570分的这根线作为评分的转换 600 = A - B*ln(1/20) 600 - 30 = A - B*ln(1/10)&emsp;&emsp;里面的1/20就是这个线性函数的x，我们用odd来表示，最后解出A和B12b = 30/np.log(2)a = 600 + b*np.log(odd) &emsp;&emsp;这样，我们就可以用客户信息对应的woe值、LR的系数、A和B来得到用户的最终分数12345678score = \a-b*intercept-\-0.783247*b*test['RuOfUL'] -\-0.382026*b*test['age'] -\-0.720417*b*test['30-59DaysPastDue'] -\-0.182033*b*test['DebtRatio'] -\-0.543901*b*test['MIncome'] -\-0.603020*b*test['Dependents'] &emsp;&emsp;我们来画一下这些用户得分的分布情况&emsp;&emsp;这个直方图有三座山峰，感觉还挺符合现状的，很大部分人评分大于550，属于信用良好客户；其次是分数在450-550，属于普通用户，这些人可能存在失信的风险；剩下的450分以下的属于准老赖用户，失信概率较高。&emsp;&emsp;前面评分的地方我是用自己的话来讲的，可能有地方不对，有大佬觉得有问题可以在下面评论纠正一下哈～&emsp;&emsp;到目前为止，就完成了对用户初步的评分，之后还有很多工作要做，比如调整模型，根据其他的指标调整分数等等，因为不是很熟悉这个领域，所以就先做到这里吧～]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下fcitx的配置和各种问题（图标异常、跳光标、卡顿等）]]></title>
    <url>%2F2018%2F10%2F30%2Ffcitx%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;相信每个第一次用linux的用户一开始都会碰到输入法的问题吧。大部分系统用的时fcitx，而刚装好系统后发现，这个小企鹅输入法怎么用不了。确实，就是不能直接给你用，它只是一个安装了基本结构的输入法，剩余的东西需要你自己手动来装。 安装一个ui&emsp;&emsp;首先你需要一个ui，这样你才能’看到’你的输入法，直接安装fcitx-ui-classic，之后注销一下就能看到你的输入法并且使用了。1sudo apt install fcitx-ui-classic 其他组件&emsp;&emsp;其它组件可能不同系统预安装的不一样吧，我们可以用fcitx-diagnose来查看我们的输入法还有哪些问题，可能现在你觉得你的输入法没有问题可以正常使用，但其实还是有很多东西不全，未来可能会出现一些bug，比如标题说的图标异常（可以大到遮天蔽日）和跳光标（你打拼音’biaji’，可能他给你拼’baji’，那个i早就给你打出来了)，还可能突然一下输入法闪退等等。&emsp;&emsp;所以我们需要用fcitx-diagnose命令来看一下还有哪些地方没配置好，其中一个组件很可能需要安装：fcitx-frontend-all，它包括了所有frontend组件包括fbterm、gtk2、gtk3、qt4、qt5。然后可以修改一下配置文件，来指定使用某些组件12345678cp /etc/X11/xinit/xinitrc ~/.xinitrc#然后在后面加上export XMODIFIERS='@im=fcitx'export XIM=fcitxexport XIM_PROGRAM=/usr/bin/fcitxexport GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxfcitx &amp; &emsp;&emsp;总之有问题先去看fcitx-diagnose里哪些环境不满足，然后做相应的调整，可能你遇到的问题和我不一样，有些地方看不明白，善用网络资源找到相应修改办法就好了，反正大概都是做上面的这些事情。 cloudpinyin&emsp;&emsp;你可以直接安装搜狗拼音来获得类似windows下的输入体验，当然如果它在你的系统上有bug或者没有适合你发行版的安装包（目前只有deepin和ubuntu的安装包，当然他们的爸爸妈妈兄弟姐妹也可以用，只是可能有一些bug），可能用起来不是那么爽，很大一部分原因，它的词库太小，我们可以导入更大的词库，但是我们更想要像windows上的输入法一样有一个云检索。&emsp;&emsp;实现方法很简单，直接安装fcitx-cloudpinyin就可以了，然后在fcitx的插件设置里把source设置为Baidu就行，特地说这个只是因为我发现很多人不知道有这么个东西。 sunpinyin卡顿？&emsp;&emsp;卡顿应该要说分两种，一种是你碰到了bug，比如输入的一些东西刚好遇上bug，这种卡顿时暂时性的。但也有一种卡顿，不管你打什么都卡。。。这种卡顿我相信不少同学都经历过。这个问题是因为输入法每次访问词库时，实际上时通过检索数据库来实现的，慢就慢在这个读写上，所以你打得慢还好，你打的一快检索匹配的速度跟不上你单身多年的手速就卡了。肯定也有的同学表示没有这种事呀，如果你的系统在固态上安装，读写速度跟得上就能好很多，自然不会卡。其实本来这些内容应该加载到内存上就没有这个问题的呀，毕竟内存这么快，但是不好意思它就是没有，所以我们手动把它给整上去。&emsp;&emsp;方法我是百度来的，它是每次开机把词库放入内存，关机前把词库写回来12345678910111213141516171819202122232425262728mv ~/.sunpinyin/userdict ~/.sunpinyin/userdict.newcp ~/.sunpinyin/userdict.new /dev/shm/userditc# 添加以下命令至/etc/rc.local，实现开机启动（写在exit 0前）cp ～/.sunpinyin/userdict.new /dev/shm/userdictchmod 777 /dev/shm/userdictln -s -f /dev/shm/userdict ～/.sunpinyin/# 较新的系统可能已经取消了rc.local，但还可以手动做一个出来# 先写一个一样的rc.local文件出来，如果没看过的同学，你可以直接里面写上面的命令最后加一行exit 0# 然后把它添加进服务chmod +x /etc/rc.localsystemctl daemon-reloadsystemctl start rc-local# 在/etc/crontab里添加下面一行，以实现自动备份词库(这里表示每1小时备份一次)00 */1 * * * user cp /dev/shm/userdict ～/.sunpinyin/userdict.new# 在/etc/init.d/backup_sunpinyin添加下面内容，关机自动备份词库cp /dev/shm/userdict ～/.sunpinyin/userdict.new# 剩余配置sudo chown root:root /etc/init.d/backup_sunpinyinsudo chmod 744 /etc/init.d/backup_sunpinyinsudo chmod +x /etc/init.d/backup_sunpinyinsudo ln -s /etc/init.d/backup_sunpinyin /etc/rc0.d/K99backup_sunpinyinsudo ln -s /etc/init.d/backup_sunpinyin /etc/rc6.d/K99backup_sunpinyin &emsp;&emsp;这个方法亲测有效，当然有条件的同学直接把系统装在固态上吧，我感觉比机械硬盘上爽太多了。]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>fcitx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下n卡驱动配置（听说你n卡发热量大？）]]></title>
    <url>%2F2018%2F10%2F30%2Fnvidia%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;总是能听到很多人说，linux下n卡怎么不好，不好装不好用，可能很多人还保留着很早以前对nvidia的怨气吧。实际使用起来还不错，只是我最早使用时确实风扇有点闹腾，但也很好解决。&emsp;&emsp;我按照debian wiki上的方法并不能装好，也是觉得很奇怪，所以只能下官网的安装包来解决了。&emsp;&emsp;所以，下面我将的都是用nvidia官网安装包进行安装的步骤，而不是用系统软件库来安装。事实上，两者差别不大，要说差别可能在于，你从软件库下载的就几个版本而nvidia官网有很多版本可以选，系统官网一般会说最好根据软件库的版本安装，因为这个版本可能是他们认为比较适合目前系统版本，并且如果有更新也可以跟着官方一起更新，这样可以更稳定一些。但是谁叫我试了几次都不行呢（ubuntu下傻瓜式的倒是好装）。这个方法对所有发行版都是通用的，我目前用的是debian9.5。&emsp;&emsp;首先，我们到nvidia官网根据系统和显卡选好驱动下载，选较新的长期维护版就好了，下下来是一个run文件，直接在root权限下运行就行。&emsp;&emsp;不过在安装之前需要一些准备～ 关闭nouveau驱动&emsp;&emsp;这是n卡的开源驱动，因为nvidia在很多地方没有开源，所以就算官方做的再不用心也比开源版的好用很多。在安装前先要禁用这个驱动，直接到/etc/modprobe.d/blacklist.conf下添加blacklist nouveau即可。 安装依赖包&emsp;&emsp;我在安装驱动前，需要gcc、make还有linux-headers，linux-headers根据当前系统内核安装就行，内核版本其实在开机时就能看到，或者用screenfetch之类的也可以看到，比如我要安装的是linux-headers-4.9.0-7-all-amd641sudo apt install gcc make linux-headers-4.9.0-7-all-amd64 建立32位架构&emsp;&emsp;相信大部分人的系统都是64位的，很多软件本身就需要32位架构，也需要n卡驱动安装32位架构相应的部分，所以还是之前先搞定这个问题。要不然等到某个软件因为没有32位显卡驱动而报错，甚至有的软件只是默默不显示特效但也不报错时，你还是得重装一遍。&emsp;&emsp;在debian下，按照下面的来就行，其他发行版可能包的名字不一样，百度一下就好了123sudo dpkg --add-architecture i386sudo apt updatesudo apt install lib32z1 lib32ncurses5 安装驱动&emsp;&emsp;接下来就直接安装驱动了，安装前记住关闭图形界面，可以直接sudo init 3，然后直接运行驱动文件即可123456sudo init 3;# 然后随便ctrl+alt+(1-6)进入cli，登录# 进入驱动文件所在目录chmod +x ./NVIDIA***.runsudo ./NVIDIA***.run &emsp;&emsp;接下来就时傻瓜式安装了，如果系统是debian用我的方法应该没有问题，如果其他发行版有报错见招拆招就行了。&emsp;&emsp;安装好后重启，就可以看到驱动搞定了。nvidia x setting可以正常使用，如果有第二块屏幕也可以亮起来了。 限制显卡频率&emsp;&emsp;相信很多用过n卡的人都有体验，好像很发热呀，我发现如果把系统装在固态上会好蛮多。&emsp;&emsp;其实它疯狂发热，并不是驱动和软件兼容性不好这样的问题，而是它的策略问题。当有大任务时，显卡把频率升高是很正常的，但是问题就是，当运算量下降后，它并不会很快把频率降下来，而是要等45秒还是50秒，所以如果每分钟实际只有几秒需要高速运转，你的显卡就一直时高负荷运转了。我们要做的，就是设置它控制频率的策略，我是直接选最节能，也就是把频率限制在最低档，完全不影响日常使用，当然也可以选择根据温度来限制显卡性能。不过如果你需要在linux上经常玩大游戏或者用gpu加速，可能就不用这样做了吧。&emsp;&emsp;直接找到/etc/X11/xorg.conf，在Section “Device”下添加下面内容：12Option "Coolbits" "28"Option "RegistryDwords" "PowerMizerEnable=0x1; PerfLevelSrc=0x2222; PowerMizerLevel=0x3; PowerMizerDefault=0x3; PowerMizerDefaultAC=0x3" &emsp;&emsp;这样就可以了：&emsp;&emsp;如果你之前显卡风扇疯狂转，重启之后你电脑的表现可能让你大吃一惊，我的显卡在夏天的温度好像都没超过44度，续航时间比在windows下还要长（另外推荐可以使用tlp），并且依然使用很流畅。里面参数的意义可以去nvidia官方文档、论坛或者arch wiki里也有很多说明。其实coolbits不用28，因为我们需要的功能不多。用这个方法可以限制频率，也可以实现超频，相应的也能定义风扇转速，不过一般的笔记本用户还是别玩这个了，小心大力出奇迹哦。]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建立ip池]]></title>
    <url>%2F2018%2F10%2F30%2Fip-pool%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;每当要爬取网站数据时，最大的麻烦就是网站的反爬虫，其中封ip就是几乎每个网站会做的事情。&emsp;&emsp;这时我们就需要一些代理ip，因为没钱也不认识人，就只能爬一些免费的ip来用，基本上免费的代理质量都堪忧，西刺还能有一些ip能用，但也很差，不过还是先做一个简易的ip池以供不备之需吧。&emsp;&emsp;说它简易，是因为真的简单。。。就是把网站上的ip爬下来，存进数据库，然后通过检验过滤掉不能用的ip。 爬取ip&emsp;&emsp;爬取部分的代码，主要分两部分，一部分检查数据库里ip在网站显示的时间，这样我们只要爬到这个时间后面的最新的ip就可以停止了；第二部分是爬取网站里相关信息：协议类型(http、https)、ip地址、显示时间并存入数据库&emsp;&emsp;西刺直接用requests请求会直接被拒绝，因为不会多的，我就只能用selenium来获取网页了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import requestsfrom bs4 import BeautifulSoup as bsfrom selenium import webdriverimport timeimport mysql.connector as mc#爬https到spider.proxy，数据库和表之前已经建好了#这里只爬了https的ip，其他类型的也是一样操作conn = mc.connect(user='yourname', password='***', database='spider')cursor = conn.cursor()cursor.execute('select time from proxy order by time desc limit 1;')time_bf = cursor.fetchall() last_time = time_bf[0][0]print('last time:', last_time)switch=Falsefor i in range(1, 101): options = webdriver.FirefoxOptions() options.set_headless() driver = webdriver.Firefox(firefox_options=options) driver.get('http://www.xicidaili.com/wn/%d'%i) soup = bs(driver.page_source, 'lxml') mass = soup.find_all('tr')[1:] if switch: driver.close() print('updated!!') break for item in mass: masses = list(item.children) ip = 'https://' + masses[3].string + ':' + masses[5].string t = masses[19].string if t &lt; last_time: switch=True #当某条ip的时间比数据库里最早的还早就跳出循环 break cursor.execute('insert into proxy\ (type, ip, time)\ values\ ("https", %s, %s);'%(repr(ip), repr(t))) time.sleep(3) driver.close() print('success page'+str(i))conn.commit()conn.close() 检验ip是否能用 &emsp;&emsp;我这里只删除掉了连接不上的，其实免费的ip都不稳定，这次延迟一两秒，下次可能八九秒，也没必要根据延迟时间来筛选&emsp;&emsp;接下来就多线程对这些ip进行测试，其实应该用异步更快，但是我对python异步的使用还不大熟，以后有机会再完整地学一下。 123456789101112131415161718192021222324252627282930313233343536373839import requestsimport mysql.connector as mcimport randomimport threadpoolagent = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", ... ]# 这里我就写两个，可以加很多conn = mc.connect(user='yourname', password='***', database='spider')cursor = conn.cursor()cursor.execute('select ip from proxy;')ip_list = cursor.fetchall()def test(ip): proxies = &#123;'https':ip[0]&#125; headers = &#123;'User_Agent':random.choice(agent)&#125; try: r = requests.get('https://movie.douban.com/', proxies=proxies, headers=headers, timeout=10) if r.status_code == 200: print('success', ip[0], r.elapsed.microseconds) else: cursor.execute('delete from proxy where ip=%s;'%repr(ip[0])) print('delete',ip[0]) except: cursor.execute('delete from proxy where ip=%s;'%repr(ip[0])) print('delete',ip[0])# 创建进程池批量测试ippool = threadpool.ThreadPool(10)rqs = threadpool.makeRequests(test, ip_list)[pool.putRequest(req) for req in rqs]pool.wait()conn.commit()conn.close() &emsp;&emsp;到这里，一个简易的ip池就建好了，每次需要用时，先收集新的ip然后对新老ip进行测试筛选就行了。]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>ip池</tag>
      </tags>
  </entry>
</search>
