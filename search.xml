<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux下fcitx的配置和各种问题（图标异常、跳光标、卡顿等）]]></title>
    <url>%2F2018%2F10%2F30%2Ffcitx%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;相信每个第一次用linux的用户一开始都会碰到输入法的问题吧。大部分系统用的时fcitx，而刚装好系统后发现，这个小企鹅输入法怎么用不了。确实，就是不能直接给你用，它只是一个安装了基本结构的输入法，剩余的东西需要你自己手动来装。 安装一个ui&emsp;&emsp;首先你需要一个ui，这样你才能’看到’你的输入法，直接安装fcitx-ui-classic，之后注销一下就能看到你的输入法并且使用了。1sudo apt install fcitx-ui-classic 其他组件&emsp;&emsp;其它组件可能不同系统预安装的不一样吧，我们可以用fcitx-diagnose来查看我们的输入法还有哪些问题，可能现在你觉得你的输入法没有问题可以正常使用，但其实还是有很多东西不全，未来可能会出现一些bug，比如标题说的图标异常（可以大到遮天蔽日）和跳光标（你打拼音’biaji’，可能他给你拼’baji’，那个i早就给你打出来了)，还可能突然一下输入法闪退等等。&emsp;&emsp;所以我们需要用fcitx-diagnose命令来看一下还有哪些地方没配置好，其中一个组件很可能需要安装：fcitx-frontend-all，它包括了所有frontend组件包括fbterm、gtk2、gtk3、qt4、qt5。然后可以修改一下配置文件，来指定使用某些组件12345678cp /etc/X11/xinit/xinitrc ~/.xinitrc#然后在后面加上export XMODIFIERS='@im=fcitx'export XIM=fcitxexport XIM_PROGRAM=/usr/bin/fcitxexport GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxfcitx &amp; &emsp;&emsp;总之有问题先去看fcitx-diagnose里哪些环境不满足，然后做相应的调整，可能你遇到的问题和我不一样，有些地方看不明白，善用网络资源找到相应修改办法就好了，反正大概都是做上面的这些事情。 cloudpinyin&emsp;&emsp;你可以直接安装搜狗拼音来获得类似windows下的输入体验，当然如果它在你的系统上有bug或者没有适合你发行版的安装包（目前只有deepin和ubuntu的安装包，当然他们的爸爸妈妈兄弟姐妹也可以用，只是可能有一些bug），可能用起来不是那么爽，很大一部分原因，它的词库太小，我们可以导入更大的词库，但是我们更想要像windows上的输入法一样有一个云检索。&emsp;&emsp;实现方法很简单，直接安装fcitx-cloudpinyin就可以了，然后在fcitx的插件设置里把source设置为Baidu就行，特地说这个只是因为我发现很多人不知道有这么个东西。 sunpinyin卡顿？&emsp;&emsp;卡顿应该要说分两种，一种是你碰到了bug，比如输入的一些东西刚好遇上bug，这种卡顿时暂时性的。但也有一种卡顿，不管你打什么都卡。。。这种卡顿我相信不少同学都经历过。这个问题是因为输入法每次访问词库时，实际上时通过检索数据库来实现的，慢就慢在这个读写上，所以你打得慢还好，你打的一快检索匹配的速度跟不上你单身多年的手速就卡了。肯定也有的同学表示没有这种事呀，如果你的系统在固态上安装，读写速度跟得上就能好很多，自然不会卡。其实本来这些内容应该加载到内存上就没有这个问题的呀，毕竟内存这么快，但是不好意思它就是没有，所以我们手动把它给整上去。&emsp;&emsp;方法我是百度来的，它是每次开机把词库放入内存，关机前把词库写回来12345678910111213141516171819202122232425262728mv ~/.sunpinyin/userdict ~/.sunpinyin/userdict.newcp ~/.sunpinyin/userdict.new /dev/shm/userditc# 添加以下命令至/etc/rc.local，实现开机启动（写在exit 0前）cp ～/.sunpinyin/userdict.new /dev/shm/userdictchmod 777 /dev/shm/userdictln -s -f /dev/shm/userdict ～/.sunpinyin/# 较新的系统可能已经取消了rc.local，但还可以手动做一个出来# 先写一个一样的rc.local文件出来，如果没看过的同学，你可以直接里面写上面的命令最后加一行exit 0# 然后把它添加进服务chmod +x /etc/rc.localsystemctl daemon-reloadsystemctl start rc-local# 在/etc/crontab里添加下面一行，以实现自动备份词库(这里表示每1小时备份一次)00 */1 * * * user cp /dev/shm/userdict ～/.sunpinyin/userdict.new# 在/etc/init.d/backup_sunpinyin添加下面内容，关机自动备份词库cp /dev/shm/userdict ～/.sunpinyin/userdict.new# 剩余配置sudo chown root:root /etc/init.d/backup_sunpinyinsudo chmod 744 /etc/init.d/backup_sunpinyinsudo chmod +x /etc/init.d/backup_sunpinyinsudo ln -s /etc/init.d/backup_sunpinyin /etc/rc0.d/K99backup_sunpinyinsudo ln -s /etc/init.d/backup_sunpinyin /etc/rc6.d/K99backup_sunpinyin &emsp;&emsp;这个方法亲测有效，当然有条件的同学直接把系统装在固态上吧，我感觉比机械硬盘上爽太多了。]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>fcitx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下n卡驱动配置（听说你n卡发热量大？）]]></title>
    <url>%2F2018%2F10%2F30%2Fnvidia%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;总是能听到很多人说，linux下n卡怎么不好，不好装不好用，可能很多人还保留着很早以前对nvidia的怨气吧。实际使用起来还不错，只是我最早使用时确实风扇有点闹腾，但也很好解决。&emsp;&emsp;我按照debian wiki上的方法并不能装好，也是觉得很奇怪，所以只能下官网的安装包来解决了。&emsp;&emsp;所以，下面我将的都是用nvidia官网安装包进行安装的步骤，而不是用系统软件库来安装。事实上，两者差别不大，要说差别可能在于，你从软件库下载的就几个版本而nvidia官网有很多版本可以选，系统官网一般会说最好根据软件库的版本安装，因为这个版本可能是他们认为比较适合目前系统版本，并且如果有更新也可以跟着官方一起更新，这样可以更稳定一些。但是谁叫我试了几次都不行呢（ubuntu下傻瓜式的倒是好装）。这个方法对所有发行版都是通用的，我目前用的是debian9.5。&emsp;&emsp;首先，我们到nvidia官网根据系统和显卡选好驱动下载，选较新的长期维护版就好了，下下来是一个run文件，直接在root权限下运行就行。&emsp;&emsp;不过在安装之前需要一些准备～ 关闭nouveau驱动&emsp;&emsp;这是n卡的开源驱动，因为nvidia在很多地方没有开源，所以就算官方做的再不用心也比开源版的好用很多。在安装前先要禁用这个驱动，直接到/etc/modprobe.d/blacklist.conf下添加blacklist nouveau即可。 安装依赖包&emsp;&emsp;我在安装驱动前，需要gcc、make还有linux-headers，linux-headers根据当前系统内核安装就行，内核版本其实在开机时就能看到，或者用screenfetch之类的也可以看到，比如我要安装的是linux-headers-4.9.0-7-all-amd641sudo apt install gcc make linux-headers-4.9.0-7-all-amd64 建立32位架构&emsp;&emsp;相信大部分人的系统都是64位的，很多软件本身就需要32位架构，也需要n卡驱动安装32位架构相应的部分，所以还是之前先搞定这个问题。要不然等到某个软件因为没有32位显卡驱动而报错，甚至有的软件只是默默不显示特效但也不报错时，你还是得重装一遍。&emsp;&emsp;在debian下，按照下面的来就行，其他发行版可能包的名字不一样，百度一下就好了123sudo dpkg --add-architecture i386sudo apt updatesudo apt install lib32z1 lib32ncurses5 安装驱动&emsp;&emsp;接下来就直接安装驱动了，安装前记住关闭图形界面，可以直接sudo init 3，然后直接运行驱动文件即可123456sudo init 3;# 然后随便ctrl+alt+(1-6)进入cli，登录# 进入驱动文件所在目录chmod +x ./NVIDIA***.runsudo ./NVIDIA***.run &emsp;&emsp;接下来就时傻瓜式安装了，如果系统是debian用我的方法应该没有问题，如果其他发行版有报错见招拆招就行了。&emsp;&emsp;安装好后重启，就可以看到驱动搞定了。nvidia x setting可以正常使用，如果有第二块屏幕也可以亮起来了。 限制显卡频率&emsp;&emsp;相信很多用过n卡的人都有体验，好像很发热呀，我发现如果把系统装在固态上会好蛮多。&emsp;&emsp;其实它疯狂发热，并不是驱动和软件兼容性不好这样的问题，而是它的策略问题。当有大任务时，显卡把频率升高是很正常的，但是问题就是，当运算量下降后，它并不会很快把频率降下来，而是要等45秒还是50秒，所以如果每分钟实际只有几秒需要高速运转，你的显卡就一直时高负荷运转了。我们要做的，就是设置它控制频率的策略，我是直接选最节能，也就是把频率限制在最低档，完全不影响日常使用，当然也可以选择根据温度来限制显卡性能。不过如果你需要在linux上经常玩大游戏或者用gpu加速，可能就不用这样做了吧。&emsp;&emsp;直接找到/etc/X11/xorg.conf，在Section “Device”下添加下面内容：12Option "Coolbits" "28"Option "RegistryDwords" "PowerMizerEnable=0x1; PerfLevelSrc=0x2222; PowerMizerLevel=0x3; PowerMizerDefault=0x3; PowerMizerDefaultAC=0x3" &emsp;&emsp;这样就可以了：&emsp;&emsp;如果你之前显卡风扇疯狂转，重启之后你电脑的表现可能让你大吃一惊，我的显卡在夏天的温度好像都没超过44度，续航时间比在windows下还要长（另外推荐可以使用tlp），并且依然使用很流畅。里面参数的意义可以去nvidia官方文档、论坛或者arch wiki里也有很多说明。其实coolbits不用28，因为我们需要的功能不多。用这个方法可以限制频率，也可以实现超频，相应的也能定义风扇转速，不过一般的笔记本用户还是别玩这个了，小心大力出奇迹哦。]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建立ip池]]></title>
    <url>%2F2018%2F10%2F30%2Fip-pool%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;每当要爬取网站数据时，最大的麻烦就是网站的反爬虫，其中封ip就是几乎每个网站会做的事情。&emsp;&emsp;这时我们就需要一些代理ip，因为没钱也不认识人，就只能爬一些免费的ip来用，基本上免费的代理质量都堪忧，西刺还能有一些ip能用，但也很差，不过还是先做一个简易的ip池以供不备之需吧。&emsp;&emsp;说它简易，是因为真的简单。。。就是把网站上的ip爬下来，存进数据库，然后通过检验过滤掉不能用的ip。 爬取ip&emsp;&emsp;爬取部分的代码，主要分两部分，一部分检查数据库里ip在网站显示的时间，这样我们只要爬到这个时间后面的最新的ip就可以停止了；第二部分是爬取网站里相关信息：协议类型(http、https)、ip地址、显示时间并存入数据库&emsp;&emsp;西刺直接用requests请求会直接被拒绝，因为不会多的，我就只能用selenium来获取网页了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import requestsfrom bs4 import BeautifulSoup as bsfrom selenium import webdriverimport timeimport mysql.connector as mc#爬https到spider.proxy，数据库和表之前已经建好了#这里只爬了https的ip，其他类型的也是一样操作conn = mc.connect(user='yourname', password='***', database='spider')cursor = conn.cursor()cursor.execute('select time from proxy order by time desc limit 1;')time_bf = cursor.fetchall() last_time = time_bf[0][0]print('last time:', last_time)switch=Falsefor i in range(1, 101): options = webdriver.FirefoxOptions() options.set_headless() driver = webdriver.Firefox(firefox_options=options) driver.get('http://www.xicidaili.com/wn/%d'%i) soup = bs(driver.page_source, 'lxml') mass = soup.find_all('tr')[1:] if switch: driver.close() print('updated!!') break for item in mass: masses = list(item.children) ip = 'https://' + masses[3].string + ':' + masses[5].string t = masses[19].string if t &lt; last_time: switch=True #当某条ip的时间比数据库里最早的还早就跳出循环 break cursor.execute('insert into proxy\ (type, ip, time)\ values\ ("https", %s, %s);'%(repr(ip), repr(t))) time.sleep(3) driver.close() print('success page'+str(i))conn.commit()conn.close() 检验ip是否能用 &emsp;&emsp;我这里只删除掉了连接不上的，其实免费的ip都不稳定，这次延迟一两秒，下次可能八九秒，也没必要根据延迟时间来筛选&emsp;&emsp;接下来就多线程对这些ip进行测试，其实应该用异步更快，但是我对python异步的使用还不大熟，以后有机会再完整地学一下。 123456789101112131415161718192021222324252627282930313233343536373839import requestsimport mysql.connector as mcimport randomimport threadpoolagent = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", ... ]# 这里我就写两个，可以加很多conn = mc.connect(user='yourname', password='***', database='spider')cursor = conn.cursor()cursor.execute('select ip from proxy;')ip_list = cursor.fetchall()def test(ip): proxies = &#123;'https':ip[0]&#125; headers = &#123;'User_Agent':random.choice(agent)&#125; try: r = requests.get('https://movie.douban.com/', proxies=proxies, headers=headers, timeout=10) if r.status_code == 200: print('success', ip[0], r.elapsed.microseconds) else: cursor.execute('delete from proxy where ip=%s;'%repr(ip[0])) print('delete',ip[0]) except: cursor.execute('delete from proxy where ip=%s;'%repr(ip[0])) print('delete',ip[0])# 创建进程池批量测试ippool = threadpool.ThreadPool(10)rqs = threadpool.makeRequests(test, ip_list)[pool.putRequest(req) for req in rqs]pool.wait()conn.commit()conn.close() &emsp;&emsp;到这里，一个简易的ip池就建好了，每次需要用时，先收集新的ip然后对新老ip进行测试筛选就行了。]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>ip池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[house_price(2)建模]]></title>
    <url>%2F2018%2F10%2F28%2Fhouse-price2%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;这里我使用了ridge、lasso、xgboost，然后对他们进行stacking，最后把这四个的预测值加权求平均值12345678from sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import RobustScalerfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lassofrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import GridSearchCVfrom xgboost import XGBRegressorfrom mlxtend.regressor import StackingCVRegressor ridge&emsp;&emsp;先手动找一下ridge比较好的l2值，这里是先粗略看下分布，后面在15附近又搜索了一遍，确定alpha=15.21234567891011def ridge_test(alpha): reg = make_pipeline(RobustScaler(), Ridge(alpha=alpha)) score = np.sqrt(-cross_val_score(reg, x_train, y_train, scoring='neg_mean_squared_error', cv=10)).mean() return scoreridge_scores = []alphas = list(range(1,30))+[40,50,60]for alpha in alphas: ridge_scores.append(ridge_test(alpha))plt.plot(alphas, ridge_scores) &emsp;&emsp;可以看到在15附近rmse值最低，最后确定为15.21ridge = make_pipeline(RobustScaler(), Ridge(alpha=15.2)) #lasso&emsp;&emsp;用类似上面的方法找lasso模型比较合适的正则化系数1234567891011lasso_scores = []def lasso_test(alpha): lasso = make_pipeline(RobustScaler(), Lasso(alpha=alpha, max_iter=1e7)) lasso_scores.append(\ np.sqrt(-cross_val_score(lasso, x_train, y_train, cv=5, scoring='neg_mean_squared_error')).mean())alphas = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]for alpha in alphas: lasso_test(alpha)plt.plot(alphas, lasso_scores)plt.show() &emsp;&emsp;最后选择alpha=0.00041lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0004, max_iter=1e7)) xgboost&emsp;&emsp;xgb需要用gridsearch来挑选参数，我是根据google到的一个顺序来挑选参数，但我感觉这个方法不够’优秀’，因为每次只对一到两个参数做gridsearch，会错过很多各个参数的搭配，但是一次调试的参数多起来，电脑吃不消。不知道是有更多调试的技巧，还是高手都是用服务器或者还有其他加速方式以供他们搜索大量的情况，以后有机会要请教一下&emsp;&emsp;这是后来缩小范围的gridsearch：1234567891011gsearch = GridSearchCV(estimator = XGBRegressor(), param_grid=&#123; 'max_depth':[3,4,5], 'subsample':[0.5,0.6,0.7], 'colsample_bytree':[0.5,0.6,0.7], 'min_child_weight':[0,1,2], 'gamma':[0,0.003,0.005], 'seed':list(range(20)) &#125;, scoring='neg_mean_squared_error', cv=5) &emsp;&emsp;最后：123xgb = make_pipeline(RobustScaler(), XGBRegressor(learning_rate =0.01, n_estimators=3500, max_depth=3, min_child_weight=0 , gamma=0, subsample=0.6, colsample_bytree=0.6, reg_alpha=0, seed=17)) stacking&emsp;&emsp;通过对上面3个模型的预测值再进行学习，得到新的模型。同时这个方法有个参数use_features_in_secondary，如果选中，还会将原始数据放进来学习，这里我选True，元分类器我选择之前的xgb1stack = StackingCVRegressor(regressors=(ridge, lasso, xgb), meta_regressor=xgb, use_features_in_secondary=True) 汇总结果&emsp;&emsp;最后用这4个模型对测试集进行预测，并将结果加权平均（记得把y用np.expm1转化回来）12345678910111213ridge.fit(x_train, y_train)y_ridge = np.expm1(ridge.predict(x_test))lasso.fit(x_train, y_train)y_lasso = np.expm1(lasso.predict(x_test))xgb.fit(x_train, y_train)y_xgb = np.expm1(xgb.predict(x_test))stack.fit(x_train, y_train)y_stack = np.expm1(stack.predict(x_test))predict = 0.5*y_stack + 0.1*y_ridge + 0.2*y_lasso + 0.2*y_xgb &emsp;&emsp;到这里，一个baseline就算完成了，剩下的就是根据理解和猜想去调整特征和模型，我做到前8%就放弃了，确实是黔驴技穷。不过说回来，入门项目虽然简单，但是能很好地让新手认识机器学习基本的流程，随着以后掌握的技能越来越多，就可以在原来的基础上提高或接触更有挑战性的项目。kaggle上的讨论和分享要比国内的这几个竞赛网站好很多，这一点也是最吸引我的地方，就看以后有没有机会回来给这个小项目来一个质的提升吧。]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>kaggle入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[house_price(1)预处理]]></title>
    <url>%2F2018%2F10%2F27%2Fhouse-price1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;这个项目是kaggle上的入门项目之一，根据房屋信息来预测房价，这也是我做的第一个项目。数据量有一些少，训练集加上测试集不到3000，所以随随便便做一下就过拟合了。。。我后来尝试调整，发现越是成绩有提升的，原来交叉验证时分数越差，但我把正则化放小后，两样成绩都变差。所以还是得在特征工程上调整&emsp;&emsp;后来我再怎么做也是定格在8%的位置，rmse值最高0.11508，我觉得用我现在的水平搞不出多大的提升。另外我还看了别人做的好的kernel，有些地方不是很理解，比如他们有的创建新变量其实原来就已经有了，添加相同的变量和修改损失函数有啥区别，但别还说，真有提升。。。所以这种层面的我可能就暂时驾驭不住了。也有人最初的分数和我差不多，但是他发现训练出来的结果在两端有一点偏差，偏大或者偏小，将两边暴力修改(乘一个系数)后成绩得到显著的提升。。。但是我的y两边没有这样，所以也没效果。感觉自己还是嫩了点，办法想不出多少，以后长本事了看能不能搞进前5%甚至前1%吧。。。&emsp;&emsp;接下来，我先把我的baseline展示一下，因为这些都是我自己理解的操作，也是一些基本操作。 大致观察&emsp;&emsp;首先我们来看看数据基本情况，看看分布，做做连续变量间相关，哪些变量有缺失值，图就不放了，同学们想象一下（其实是那个图太大了。。，其他结果直接在后面的处理里会出现）。脑补之后，我们发现了一些问题：1.一个异常年份&emsp;&emsp;一个人在车库建成年份填了2207年，有屁股的人都会觉得他可能想写2007年，去看一下相关变量，他的房子是2006年建的并且2007年得到了’强化’，没得说，就把它改成20071test['GarageYrBlt'][2593] = 2007 2.两个肉眼看上去不科学的样本&emsp;&emsp;右下角的两个点，面积在所有的里面最大，房价却算得上廉价了，最主要是他们两个偏离大众有点远，不管你是真实还是乱填的数据，都得删掉这两个毒瘤1train.drop(train[(train['GrLivArea']&gt;4500)].index, inplace=True) 3.很多缺失值&emsp;&emsp;大概有34个变量存在缺失值，就不列出来了，后面处理时自然能看到有哪些。在处理之前我想说一点，这个数据集有一个坑，很多变量里就设有NA这个变量值，用来表示没有这类东西，但是他们把缺失值也录成NA，所以在包含NA变量值的变量下，真的值是NA还是缺失了，傻傻分不清。。。我们只能根据其他变量推测这个可能确实是缺失值，没有其他信息就只能当作没缺失。下面很长一部分都是在处理缺失值： 处理缺失值 Functional&emsp;&emsp;这个变量，在数据描述里说了，如果没有就默认是typical，所以就直接这样修改吧1all_data['Functional'].fillna('Typ', inplace=1) 值域不包含NA的变量&emsp;&emsp;这些变量分别是： MasYnrType MSZoning Utilities Exterior2nd Exterior1st SaleType Electrical KitchenQual MasYnrType&emsp;&emsp;有一个样本masvnrarea值为189但masynrtype为nan，说明这个应该是缺失值，我使用’BrkFace’来填充，因为在各种单板墙里它占绝大多数且类型为brkface的单板强面积中位数也比较接近1891234567891011121314all_data['MasVnrType'].value_counts() -&gt; None 1742 BrkFace 879 Stone 247 BrkCmn 25 Name: MasVnrType, dtype: int64all_data.groupby('MasVnrType')['MasVnrArea'].median() -&gt; MasVnrType BrkCmn 161.0 BrkFace 203.0 None 0.0 Stone 200.0 Name: MasVnrArea, dtype: float64 &emsp;&emsp;其它23个nan值没看出来什么端倪，但是我想搞点事情，我觉得有单板墙比没有的可能要高档一点吧，确实没有单板墙的房子OverallQual要低一截，并且这23个房子的OverallQual在平均之上，但我在尝试用有单板墙的房子的type的众数和area平均数来填补，还不如把他们当作没有单板墙来处理（因为确实两个值都没有很有可能就是没有单板墙）123all_data['MasVnrType'].loc[2611]='BrkFace'all_data['MasVnrType'].fillna('None', inplace=1)all_data['MasVnrArea'].fillna(0, inplace=1) MSZoning&emsp;&emsp;这个我就直接用众数填充了1all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0], inplace=1) Utilities非缺失值里除了一个样本是NoSeWa其他都是allpub，并且这两个缺失了的样本供暖都是用的GasA，电力系统是FuseA，厨房质量中等，用水和下水道肯定要有吧，所以他们就是allpub1all_data['Utilities'].fillna('AllPub', inplace=1) exterior1st, exterior2nd&emsp;&emsp;就一个样本同时缺失了这两个值，首先可以肯定他们有涂料，因为ExterQual为TA，至于有没有第二种涂料就不知道了，都给他们用众数替代吧，反正绝大部分房子都没有第二种涂料12all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0], inplace=1)all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0], inplace=1) saletype, electircal&emsp;&emsp;这俩货就都用众数填充12all_data['SaleType'].fillna(all_data['SaleType'].mode()[0], inplace=1)all_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=1) kitchenqual&emsp;&emsp;厨房质量缺失的就一个样本，这个房子总体质量为5,那就用TA来填充吧1all_data['KitchenQual'].fillna('TA', inplace=1) 可以填NA的变量poolqc&emsp;&emsp;有3个样本poolqc没填但是填了poolarea，说明还是有游泳池，那就根据房屋总体质量太填充游泳池的质量，其他样本不清楚就替换成’None’123all_data.loc[2421, 'PoolQC'] = 'Fa'all_data.loc[2504, 'PoolQC'] = 'Gd'all_data.loc[561, 'PoolQC'] = 'Fa' miscfeature&emsp;&emsp;有一个样本miscfeature没填但是miscval为17000,这个还是所有miscval里最高的，而在所有miscfeature里就车库的价值最高，并且我们可以看到他的车库质量和现状处于平均水平，但是有3个车位，所以可能他有第二个车库（当然也可能是我想多了），其它nan的就用’None’替换1234567all_data[['GarageCars', 'GarageQual', 'GarageCond']].loc[2550] -&gt; GarageCars 3 GarageQual TA GarageCond TA Name: 2550, dtype: objectall_data['MiscFeature'][2550] = 'Gar2' garage系列&emsp;&emsp;和车库有关的存在缺失值的变量里，有两个不是完全缺失，他们的车库类型填的Detchd，也就是说应该有车库，那这两个样本的其他缺失值可以根据车库类型为Detchd的样本总体情况来填充12345678910all_data['GarageArea'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median()all_data['GarageCars'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageCond'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageCond'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageFinish'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageFinish'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageQual'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageQual'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].mode()[0]all_data['GarageYrBlt'][2127] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median()all_data['GarageYrBlt'][2577] = all_data['GarageArea'][all_data['GarageType']=='Detchd'].median() &emsp;&emsp;剩下garageyrblt为nan的样本都是没有车库的，就把他们都作为一类，赋值为0，本来我不是这样做的，但是分配跟它们一个年份反而结果更差basement系列&emsp;&emsp;在和地下室有关的变量里，如果一个样本缺失值超过1个，那这个样本所有的值不是0就是nan，所以可以把他们都当作没有地下室。只有一个缺失值的样本都是类比变量，可以根据其它变量来填充&emsp;&emsp;例如，BsmtFinType2根据BsmtFinSF2估计为ALQ、3个BsmtExposure缺失的地下室都还没建好，就按照绝大多数情况定为’No’、BsmtCond都用TA、BsmtQual缺失的两个值，他们的cond分别是Fa和TA，并且都光线不好所以给Fa和Po123456789all_data.loc[333, 'BsmtFinType2'] = 'ALQ'all_data.loc[949, 'BsmtExposure'] = 'No'all_data.loc[1488, 'BsmtExposure'] = 'No'all_data.loc[2041, 'BsmtCond'] = 'TA'all_data.loc[2186, 'BsmtCond'] = 'TA'all_data.loc[2218, 'BsmtQual'] = 'Fa'all_data.loc[2219, 'BsmtQual'] = 'Po'all_data.loc[2349, 'BsmtExposure'] = 'No'all_data.loc[2525, 'BsmtCond'] = 'TA' lotfrontage&emsp;&emsp;房子到街道的距离，要是在天朝像我们大部分人怕不是都只有几米，只有住在比较闲适或偏僻的地方或距离远一些，国外这个情况可能还更明显，所以可以试着根据neighborhood来填充12all_data['LotFrontage'] = \all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median())) 其它的缺失值&emsp;&emsp;最后剩下的都是暂时没看出个门道来的，所以类型变量全填充为’None’，数值型的全填充为012345all_data.loc[:, all_data.columns[all_data.dtypes=='object']] = \all_data.loc[:, all_data.columns[all_data.dtypes=='object']].fillna('None')all_data[['BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'TotalBsmtSF']] = \all_data[['BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'TotalBsmtSF']].fillna(0) 新建变量&emsp;&emsp;新增4个变量，用来描述总完成的面积、所有类型走廊总面积、是否有第二层和是否有铺置木质地板12345678all_data['total_fin_sf'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])all_data['total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])all_data['has2ndfloor'] = (all_data['2ndFlrSF']==0)*1all_data['WoodDeckSF'] = (all_data['WoodDeckSF']==0)*1 数据类型转换&emsp;&emsp;MSSubClass虽然是数值类型，但实际上各个值并不是连续的关系，所以还是转换成类别数据;变量Mosold是指售出的月份，12个月其实不存在先后顺序吧，我觉得应该变成类别数据12all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)all_data['MoSold'] = all_data['MoSold'].apply(str) 偏度处理&emsp;&emsp;对偏度大于2的特征取log(1+x)，对目标变量也是一样123456loged_features = all_data.loc[:, all_data.columns[all_data.dtypes!='object']].apply(skew)\.sort_values(ascending=False)[:12].indexall_data[loged_features] = all_data[loged_features].applymap(np.log1p)y_train = np.log1p(y_train) one-hot&emsp;&emsp;将类别变量做one-hot处理1all_data = pd.get_dummies(all_data) &emsp;&emsp;之后我没做feature selection，删少了没效果，多删一点我做cv分数有提高，但是提交上去更差。到这里就基本上对数据预处理完了，接下来主要就是套模型，找到和合适的参数，然后再做一个stacking]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>kaggle入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[credit-card(3) 建模与评分]]></title>
    <url>%2F2018%2F10%2F27%2Fcredit-card3%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;下面将对数据套用LR模型，然后根据模型参数和woe值对样本进行评分，这个评分只是一个初步的评分，后续还有许多工作需要做来调整这个评分，不过我不是很了解这些，所以先就当一个初级工吧。&emsp;&emsp;首先我们留出30%的数据用以评估模型:12X_train, X_test, y_train, y_test = train_test_split( train.iloc[:,:-1], train['target'], test_size=0.3, random_state=42) 再次处理不平衡&emsp;&emsp;之前的处理我只把正负样本比例升到10:1，因为这个方法容易导致过拟合，但实际上10:1的比例还是有点高，就算模型全蒙是正样本都有炒鸡好的指标值，而如果直接降采样对这个数据量不是很大的数据集还是会有很多信息损失&emsp;&emsp;所以，我将随机n次进行降采样，每次随机抽取一半的正样本和所有负样本合并作为新数据集进行学习，最后对这n个数据集学习的结果求均值得到最终的结果 下采样+拼接：1234567891011def undersample(X, y, subsets=7): data = X.join(y) train_sets = [] neg = data[data['target']==0] pos = data[data['target']==1] for i in range(subsets): re_neg = neg.sample(frac=0.5) combine = pd.concat([re_neg, pos]) train_sets.append(combine) return train_setstrain_sets = undersample(X_train, y_train, subsets=7) 定义训练函数123456789101112131415161718# LR的c=2是我之前对7组数据分别交叉验证找到的较合适的参数值，因为步骤多且写出来意义不大，接直接跳过def training(data_sets, test): test_results = [] coefs = [] intercepts = [] model = LogisticRegression(C=2) for data in data_sets: y_train = data['target'] x_train = data.iloc[:,:-1] clf = model.fit(x_train.values, y_train.values) coefs.append(clf.coef_) intercepts.append(clf.intercept_) test_results.append(clf.predict_proba(test)[:,1]) return &#123;'test_results':test_results, 'coefs':coefs, 'intercepts':intercepts&#125;# 这里画不了学习曲线，在开始采样时正样本比例过低，会报错 先用70%的数据学习123456test_result = training(train_sets, X_test)results = DataFrame(test_result['test_results']).mean()from sklearn.metrics import roc_curve, aucfpr, tpr, thresholds = roc_curve(y_test.values, results.values)-&gt; auc:0.88 roc曲线ks曲线&emsp;&emsp;这个结果应该还算可以吧，用另外30%且样本分布和原来不同的数据测试还有0.88的auc，ks也有0.61，泛化能力还算凑合 用上所有的数据接下来我们把所有的数据扔进模型，最终需要的就是所有的系数，也就是斜率coefs和截距intercept1234train_sets = undersample(train.iloc[:,:-1], train['target'], subsets=7)training_result = training(train_sets, test.iloc[:, :-1])intercept = DataFrame(training_result['intercepts']).mean()[0]coefs = DataFrame(np.array(training_result['coefs'])[:,0,:]).mean() 评分&emsp;&emsp;最后就是激动人心的评分环节了。我们当然可以对每个用户用LR模型返回是坏客户的概率并基于这个概率打分，但是也可以更简单，我们使用坏客户概率和好客户概率比例的ln值，也一样能体现该用户更像好客户和坏客户，实际上就是sigmoid的反函数，这样重新变回了一个线性问题&emsp;&emsp;我们只需要用数据值（也就是woe值）以及LR的系数就可以来预测用户信用，这样实施起来也很方便，当获得用户信息时根据其所在档位woe值和系数就可以知道他的预测信用水平，以0作为分界线&emsp;&emsp;当然我们不可能直接用个分数，我们平时看到的信用值都是几百的，所以要简单线性变换一下，我这里就用当好坏比例为20和10分别对应600分和570分的这根线作为评分的转换 600 = A - B*ln(1/20) 600 - 30 = A - B*ln(1/10)&emsp;&emsp;里面的1/20就是这个线性函数的x，我们用odd来表示，最后解出A和B12b = 30/np.log(2)a = 600 + b*np.log(odd) &emsp;&emsp;这样，我们就可以用客户信息对应的woe值、LR的系数、A和B来得到用户的最终分数12345678score = \a-b*intercept-\-0.783247*b*test['RuOfUL'] -\-0.382026*b*test['age'] -\-0.720417*b*test['30-59DaysPastDue'] -\-0.182033*b*test['DebtRatio'] -\-0.543901*b*test['MIncome'] -\-0.603020*b*test['Dependents'] &emsp;&emsp;我们来画一下这些用户得分的分布情况&emsp;&emsp;这个直方图有三座山峰，感觉还挺符合现状的，很大部分人评分大于550，属于优质客户；其次是分数在450-550，属于普通用户，这些人可能存在失信的风险；剩下的450分以下的属于准老赖用户，失信概率较高。&emsp;&emsp;前面评分的地方我是用自己的话来讲的，可能有地方不对，有大佬觉得有问题可以在下面评论纠正一下哈～&emsp;&emsp;到目前为止，就完成了对用户初步的评分，之后还有很多工作要做，比如调整模型，根据其他的指标调整分数等等，因为不是很熟悉这个领域，所以就先做到这里吧～]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[credit_card(2)卡方分箱和变量筛选]]></title>
    <url>%2F2018%2F10%2F26%2Fcredit-card2%2F</url>
    <content type="text"><![CDATA[预备工作&emsp;&emsp;首先定义一个分箱函数。我这里是采用合并的方式进行分箱，对相邻的组进行卡方检验，如果显著，我这里设置的p值是0.01，则保留这两组，否则这两组合并。12345678910111213141516171819202122232425262728293031def chi2(freq, min_e=0.5): N = freq.sum().sum() r = freq.sum(axis=1) c = freq.sum(axis=0) chis = 0 for i in range(len(freq)): for j in range(len(freq.columns)): o = freq.iloc[i, j] e = r.iloc[i]*c.iloc[j]/N e = e if e &gt;= min_e else min_e chis += ((o-e)**2) / e return chisdef merge(freq, min_score=9.21): n = len(freq)-1 while True: scores = &#123;&#125; for i in range(n): score = chi2(freq.iloc[(i, i+1), :]) scores.setdefault(score, []).append((i, i+1)) floor = min(scores.keys()) if floor &lt; min_score: for left, right in list(reversed(scores[floor])): freq.iloc[left] += freq.iloc[right] freq.drop(freq.index[right], axis=0, inplace=True) n -= 1 else: break print(freq.index) print(Series(scores).sort_values()) &emsp;&emsp;因为是连续变量，而且unique值太多，从原始数据两两做卡方检验能做茫茫多年，所以我对数据先分一些组，例如比例型的值域0-1,就把他们保留两位小数，这样unique值不会超过100个(强迫症会说是101个)，对结果影响很小，但可以大大提高速度1234567891011121314151617def rd(x): if x&lt;=1: return round(x,2) elif x&lt;=10: return int(x)+1 elif x&lt;=100: return int(x/5+1)*5 elif x&lt;=1000: return int(x/100+1)*100 elif x&lt;=10000: return int(x/1000+1)*1000 elif x&lt;=100000: return int(x/10000+1)*10000 elif x&lt;=1000000: return int(x/100000+1)*100000 else: return int(x/1000000+1)*1000000 &emsp;&emsp;因为最后分箱时需要用到每个箱之间的边界，所以我就把两组间的边界定义成前一组的最大值和后一组最小值的平均数1234def get_border(name, groups): min_max = train[name].groupby(train[name].map(rd)).agg(['min', 'max']) trans = Series((min_max['min'].iloc[1:].values+min_max['max'].iloc[:-1].values)/2, index = min_max.index[1:]) return trans[groups] &emsp;&emsp;分箱后我们需要根据woe值和iv值判断变量的好坏，并且woe值将替换原数据作为新的x，所以我们也把计算woe和iv值的函数定义一下(在这个函数里我直接根据分箱把原数据替换成woe值)12345678910111213141516171819202122232425def good(x): return (len(x)-sum(x))/good_sumdef bad(x): return sum(x)/bad_sumbad_sum = int(train['target'].sum())good_sum = len(train['target']) - bad_sumivs = &#123;&#125;def woe_iv(var, cut): data = Series(train['target'].values, train[var].values) data.index = pd.cut(data.index, cut, right=False) print(data.groupby(data.index).count()) ratio = data.groupby(data.index).agg([good,bad]) woe = np.log(ratio['good']/ratio['bad']) sub = ratio['good']-ratio['bad'] iv = (sub*woe).sum() ivs[var] = iv print('woe:') print(woe, '\n') print('iv:', iv) train[var] = pd.cut(train[var], cut, right=False).map(woe.to_dict()) test[var] = pd.cut(test[var], cut, right=False).map(woe.to_dict()) print(woe.to_dict()) 开始分箱&emsp;&emsp;接下来大致套用上面的函数就可以了，但是可以根据情况进一步合并，例如有的组全是好样本但是数据量很小，可以考虑和前一组合并&emsp;&emsp;下面展示对age变量的分箱：12345678910111213141516171819202122232425262728293031323334353637freq_age = pd.crosstab(train.age, train.target)merge(freq_age)-&gt; Int64Index([21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99], dtype='int64', name='age') 21.763776 [(0, 1)] 32.122824 [(1, 2)] 12.092637 [(2, 3)] 24.292766 [(3, 4)] 23.000035 [(4, 5)] 34.589954 [(5, 6)] 14.700949 [(6, 7)] 11.499217 [(7, 8)] 133.779114 [(8, 9)] 18.759893 [(9, 10)] 58.973590 [(10, 11)] 9.492844 [(11, 12)] 12.886260 [(12, 13)] dtype: objectwoe_iv('age', [21, 23, 25, 33, 37, 48, 54, 57, 58, 63, 66, 73, 84, 99, 1000])-&gt; woe: [21, 23) 0.536632 [23, 25) -0.172970 [25, 33) -0.569851 [33, 37) -0.462247 [37, 48) -0.328169 [48, 54) -0.227933 [54, 57) -0.048145 [57, 58) 0.166081 [58, 63) 0.352834 [63, 66) 0.897501 [66, 73) 1.137055 [73, 84) 1.627105 [84, 99) 2.043198 [99, 1000) 0.083389 dtype: float64 iv: 0.354380135746 &emsp;&emsp;可以看到分箱后iv值为0.35,具有一定的预测力，适合作为变量。同时最后一个分箱woe值较小并且样本数较少，可以考虑和前一组合并（不过我还是选择保留。。）&emsp;&emsp;其它的变量也是进行类似的处理，具体的可以直接看源码&emsp;&emsp;最后再通过xgboost来排除预测能力较差的变量123456from xgboost import XGBClassifierclf = XGBClassifier()clf.fit(train.iloc[:,[0,1,2,3,4,5,6,7,9,10]], train['target'])from xgboost import plot_importanceplot_importance(clf)plt.show() &emsp;&emsp;经过对所有变量进行以上的处理，结果如下：iv&gt;1 f_name iv RuOfUL 1.84 额度使用率越高失信率越高 30-59DaysPastDue 1.64 老赖和失信是扯不开的 &emsp;&emsp;虽然iv值大于1有过于美好的怀疑，但是我感觉这两个变量还阔以用0.3&lt;iv&lt;0.5 f_name iv age 0.354 从样本分布来看还是中年人更油腻一点呀 0.1&lt;0v&lt;0.3 f_name iv debtratio 0.18 0-0.4时debtratio越高失信率越高，但0.4-1变化就不明显了，大于1的就不解释了 REL(淘汰) 0.13 本身就iv值较低，xbgoost对它的评价也很低，就抛弃这个变量了 dependents 0.24 家庭成员越多，失信率越低 MIncome 0.09 2500左右工资的人失信率高一些，除此之外还是可以看到收入越高失信率越低，只是变化幅度较小，对于iv值介于0.05到0.1的变量，可以酌情考虑保留，我就很通情达理 &emsp;&emsp;还有一些变量因为iv值过低直接排除掉，很不幸，之前自作聪明造的两个变量都不是很好&emsp;&emsp;接下来要做的就是开始建模了，需要对不平衡进行第二次处理，使用LR，最后给用户进行评分]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[credit_card (1)预处理]]></title>
    <url>%2F2018%2F10%2F26%2Fcredit-card1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;数据来自于kaggle的一个信用评估项目，比赛早就停止了，我就拿它的数据来做了一个简单的评分卡，话不多说，先来看看数据 数据预览&emsp;&emsp;数据的特征很少，总共10个自变量，下面是关于自变量的定义： 特征名 描述 RevolvingUtilizationOfUnsecuredLines Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits age Age of borrower in years NumberOfTime30-59DaysPastDueNotWorse Number of times borrower has been 30-59 days past due but no worse in the last 2 years. DebtRatio Monthly debt payments, alimony,living costs divided by monthy gross income MonthlyIncome Monthly income NumberOfOpenCreditLinesAndLoans Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards) NumberOfTimes90DaysLate Number of times borrower has been 90 days or more past due. NumberRealEstateLoansOrLines Number of mortgage and real estate loans including home equity lines of credit NumberOfTime60-89DaysPastDueNotWorse Number of times borrower has been 60-89 days past due but no worse in the last 2 years. NumberOfDependents Number of dependents in family excluding themselves (spouse, children etc.) &emsp;&emsp;目标特征则是SeriousDlqin2yrs，用0或1表示是否坏用户&emsp;&emsp;数据分布就不在博客里展示了，太难看，同学们可以自己自己画一下&emsp;&emsp;下面是各个特征相关系数图： 大致思路&emsp;&emsp;通过前面对数据的观察，我发现了几个问题，并采用以下的处理办法：1、age=0&emsp;&emsp;训练集里有一个样本age为0，属于乱填，并且这个情况在测试集里没有出现&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;删除该样本1train.drop(train[train['age']==0].index, inplace=True) 2、特征冗余&emsp;&emsp;这里的老赖分3个等级，拖了30-59天、60-89天、超过90天的，这3个变量相关度很高，说明基本都是一批人&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;保留和目标特征相关度较高的一个12train.drop(['60-89DaysPast', '90DaysLate'], axis=1, inplace=True)test.drop(['60-89DaysPast', '90DaysLate'], axis=1, inplace=True) 3、debtratio&gt;1&emsp;&emsp;这个特征是指还款、赡养费、生活开销占毛月收入比例，训练集有35137个样本大于1（测试集也不少）&emsp;&emsp;对于这个我有几个猜想：&emsp;&emsp;&emsp;&emsp;1.确实入不敷出，可能收入差的一点钱家里一起分担掉了，那么dabtratio应该接近1的&emsp;&emsp;&emsp;&emsp;2.家里有矿、有人包养，月收入几乎没有但挺会花钱的&emsp;&emsp;&emsp;&emsp;3.debtratio可能是官方根据支出和收入计算来的，而有的人收入可能乱填成很小&emsp;&emsp;&emsp;&emsp;4.收入没填的，可能系统默认当作1，因为分母不能为0（在原数据里可以看到，收入缺失的debtratio都是几位数）&emsp;&emsp;&emsp;&emsp;5.可能这个人很多信息都是乱填的&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;不管这些有的没的猜想，反正是不好直接处理，只能通过后面分箱，看能不能把大于1的样本分出几部分，如果不能就作为一类。&emsp;&emsp;另外：考虑到debtratio数据可能是后期算来的，如果debt统计正确的话，我们可能可以把它做成一个新变量12345pos_train_Income = resample_train['MIncome'].replace([0, -10000], 1)resample_train['Debt'] = pos_train_Income * resample_train['DebtRatio']pos_test_Income = test['MIncome'].replace([0, -10000], 1)test['Debt'] = pos_test_Income * test['DebtRatio'] 4、MonthlyIncome的缺失和异常值&emsp;&emsp;训练集有29731个缺失值（测试集也很多），缺失值太多，而该特征和其他特征相关度都极低，用机器学习补全也效果不好，同时也有很多很低的值，例如个位数、两位数等，我们不能否认可能部分极低值的真实性&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;月收入缺失的样本单独作为一类&emsp;&emsp;&emsp;&emsp;收入极低的样本通过后期分箱看能分出几组来12train['MIncome'].fillna(-10000, inplace=True)test['MIncome'].fillna(-10000, inplace=True) 5、RevolvingUtilizationOfUnsecuredLines&gt;1&emsp;&emsp;这个变量指信用额度的使用率，超过1肯定是有问题，有可能和debtratio一样，是因为额度统计错误等原因&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;后期分箱看能否划分出几个组6、dependents缺失&emsp;&emsp;训练集里有3924个样本在dependents上缺失，缺失量不太大，并且和age、NumberRealEstateLoansOrLines有一定相关&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;机器学习来补齐数据，这里选择randomforest1234567891011121314rf_train_x = train[train['MIncome']!=-10000].iloc[:, [1,2,3,4,6,7]]rf_train_y = train[train['MIncome']!=-10000]['Dependents']# 不想用不填收入的人，因为所有没填dependents的人都没填收入，他们虽然可能是一类人，但是他们的数据可能全都是乱写的from sklearn.ensemble import RandomForestRegressorclf = RandomForestRegressor(random_state=0, n_estimators=500, n_jobs=-1)clf.fit(rf_test_x, rf_test_y)clf.score(rf_test_x, rf_test_y)# -&gt; 0.87447288413972102# 就不做交叉验证了dep_train = np.round(clf.predict(train[train['Dependents'].isnull()].iloc[:, [1,2,3,4,6,7]]))dep_test = np.round(clf.predict(test[test['Dependents'].isnull()].iloc[:, [1,2,3,4,6,7]]))test['Dependents'][test['Dependents'].isnull()] = dep_test 7、NumberOfOpenCreditLinesAndLoans和NumberRealEstateLoansOrLines&emsp;&emsp;前者指信用贷款和抵押贷款类的贷款数量，后者指抵押贷款和不动产借贷数量&emsp;&emsp;感觉前者部分包含了后者，并且所有样本在前一个特征的值都比后一个大(让我觉得不是巧合)&emsp;&emsp;考虑建立一个新变量12345resample_train['credit'] = resample_train['OpenL2'] - resample_train['REL']test['credit'] = test['OpenL2'] - test['REL']resample_train.to_csv('train_fixed1.csv', index=False)test.to_csv('test_fixed1.csv', index=False) 8、数据不平衡&emsp;&emsp;正负样本比例约为14:1&emsp;&emsp;处理：&emsp;&emsp;&emsp;&emsp;采用smote升采样至10:1，然后多次对正样本采样生成几组训练集，对学习结果求均值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 代码是从简书上借鉴来的，用的是k临近值求均值或众数充当新数据# 这个函数有个小地方要注意# 如果一个变量unqiue值数量小于kdistinctvalue，会被认为是类别变量，会对k临近值取众数# 如果设置的较小，可能会取到k个点都不一样，这样会返回nandef smote(data, tag_index=None, max_amount=0, std_rate=5, kneighbor=5, kdistinctvalue=10, method='mean'): try: data = pd.DataFrame(data) except: raise ValueError case_state = data.iloc[:, tag_index].groupby(data.iloc[:, tag_index]).count() case_rate = max(case_state) / min(case_state) location = [] if case_rate &lt; 5: print('不需要smote过程') return data else: less_data = np.array( data[data.iloc[:, tag_index] == np.array(case_state[case_state == min(case_state)].index)[0]]) more_data = np.array( data[data.iloc[:, tag_index] == np.array(case_state[case_state == max(case_state)].index)[0]]) neighbors = NearestNeighbors(n_neighbors=kneighbor).fit(less_data) for i in range(len(less_data)): point = less_data[i, :] location_set = neighbors.kneighbors([less_data[i]], return_distance=False)[0] location.append(location_set) if max_amount &gt; 0: amount = max_amount else: amount = int(max(case_state) / std_rate) times = 0 continue_index = [] class_index = [] for i in range(less_data.shape[1]): if len(pd.DataFrame(less_data[:, i]).drop_duplicates()) &gt; kdistinctvalue: continue_index.append(i) else: class_index.append(i) case_update = list() location_transform = np.array(location) while times &lt; amount: new_case = [] pool = np.random.permutation(len(location))[1] neighbor_group = location_transform[pool] if method == 'mean': new_case1 = less_data[list(neighbor_group), :][:, continue_index].mean(axis=0) if method == 'random': away_index = np.random.permutation(len(neighbor_group) - 1)[1] neighbor_group_removeorigin = neighbor_group[1:][away_index] new_case1 = less_data[pool][continue_index] + np.random.rand() * ( less_data[pool][continue_index] - less_data[neighbor_group_removeorigin][continue_index]) new_case2 = np.array(pd.DataFrame(less_data[neighbor_group, :][:, class_index]).mode().iloc[0, :]) new_case = list(new_case1) + list(new_case2) if times == 0: case_update = new_case else: case_update = np.c_[case_update, new_case] print('已经生成了%s条新数据，完成百分之%.2f' % (times, times * 100 / amount)) times = times + 1 less_origin_data = np.hstack((less_data[:, continue_index], less_data[:, class_index])) more_origin_data = np.hstack((more_data[:, continue_index], more_data[:, class_index])) data_res = np.vstack((more_origin_data, less_origin_data, np.array(case_update.T))) label_columns = [0] * more_origin_data.shape[0] + [1] * ( less_origin_data.shape[0] + np.array(case_update.T).shape[0]) data_res = pd.DataFrame(data_res) return data_resresample_train = smote(train, tag_index=0, std_rate=10, kdistinctvalue=8) resample_train[['age', '30-59DaysPastDue', 'MIncome', 'OpenL2', 'REL', 'Dependents']] = \resample_train[['age', '30-59DaysPastDue', 'MIncome', 'OpenL2', 'REL', 'Dependents']].applymap(round)resample_train['MIncome'].replace(-6000, -10000, inplace=True) #用smote造出的收入有-10000和-6000，依然改为-10000 &emsp;&emsp;一个简单的数据预处理就做完了，接下来就是对变量进行卡方分箱]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>评分卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉钩网数据分析师岗位分析]]></title>
    <url>%2F2018%2F10%2F22%2Flagou%2F</url>
    <content type="text"><![CDATA[div#salary{ width:100%; height:30vw; margin:auto; } div#flex_h{ display: flex; justify-content:center; width: 80%; position:relative; left: 10%; margin: 2px; } div#flex_v{ display: flex; flex-direction: column; width: 100%; } div.echarts{ width: 100%; height: 30vw; margin:auto; } div#comment1{ width: 100%; margin:auto; } div#comment2{ width: 100%; margin:auto; } button{ background-color: white; border: 2px solid #555555; margin: 4px 2px; } button:hover { background-color: #555555; color: white; } img { width: 70%;} 前言&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;之前因为需要抉择去广州还是深圳找工作，爬取了一些数据，顺便做了个简单的数据分析，旨在了解数据分析岗位的信息以及两个城市间的差别，我会尽量少说废话，多用图来展示信息，里面应该会有一些不准确的理解和推测，所以只供参考，更多的还是让数据自己说话&emsp;&emsp;&emsp;&emsp;本来之前就打算找工作，但是因为一些事情我现在才开始去找工作，所以数据有点早，是8月份的数据&emsp;&emsp;&emsp;&emsp; 分析过程 使用python爬取拉勾网关于数据分析的岗位信息，并存入数据库 使用selenium爬取职位信息以及详情网页，通过requests库爬取具体信息，并通过高德地图api获取公司经纬度（拉勾网的地理位置就是用高德地图定位的） 使用BeautifulSoup库解析html 调用mysql的api将数据存入数据库 通过numpy和pandas对数据进行各类描述性统计分析 对关键词使用python的WordCloud包生成词云，需要用到jieba分词 词云我简单做成了方形，是有点丑。。。 使用百度的echarts对数据进行可视化 对中间有一部分的图，因为比较多，所以我把他们放在一个div里，通过点击按钮来切换，节省空间，观感也更好 下面开始展示分析结果： 公司地理分布及其岗位工资&emsp;&emsp;将两个城市提供数据分析岗位的公司地理位置在地图上标出，并通过颜色反映相应工资：&emsp;&emsp;紫色波浪边缘的点表示该城市工资排名前十的公司，其余点根据颜色表示岗位工资水平，越接近红色越高，越接近绿色越低&emsp;&emsp;将鼠标悬浮在左下角的单位标记上的某个位置从而只显示相应工资水平的公司位置，也可以拖动调整其上下界，来限制只显示某个范围工资的公司&emsp;&emsp;将鼠标悬浮在地图上的点或行政区上，从而查看该公司提供岗位的工资或该行政区的平均工资&emsp;&emsp;可以鼠标拖动或滚轮缩放地图 &emsp;&emsp;在深圳绝大部分公司分布在南山区和福田区，平均工资也是前2,分别为18.54、19.46，远高于其他地区并且工资top10的公司也都在这里&emsp;&emsp;其次罗湖区和宝安区也有少量分布，剩余地方就只有零零星星的岗位了&emsp;&emsp;看来如果去深圳找工作，主要就是在南山区和福田区了&emsp;&emsp;在广州，天河区的岗位数量最多，但是平均工资13.06，标准差较大&emsp;&emsp;其次越秀区、荔湾区、海珠区、番禺区、黄埔区也有一定数量的岗位，并且荔湾、海珠、番禺的平均工资都比天河区高，越秀区平均工资最低，只有10.34，黄埔区紧追其次平均工资12.38&emsp;&emsp;白云区只有三个岗位，平均工资15.75&emsp;&emsp;工资前十的公司有6个在天河，1个在越秀，2个在荔湾，1个在番禺&emsp;&emsp;感觉天河承包了广州数据分析的半壁江山薪资分布&emsp;&emsp;&emsp;&emsp;对所有岗位的薪资保留2位小数，然后用曲线图描述两个城市在各个薪资水平上的分布数量&emsp;&emsp;深圳20k以上的岗位比广州多，20k以下差别不明显，所以两个城市薪资大体分布接近&emsp;&emsp;两个城市的工资肉眼上可以分出四档：&emsp;&emsp;&emsp;&emsp; 5k-15k&emsp;&emsp;&emsp;&emsp; 20k-23k&emsp;&emsp;&emsp;&emsp; 30k左右&emsp;&emsp;&emsp;&emsp; 40k左右&emsp;&emsp;第一档占了大部分的岗位，也是大部分人入行的方向&emsp;&emsp;第二档和第一档之间有一个明显的空档，所以应该对技能和经验有高一阶的要求&emsp;&emsp;第三档和第四档可能是给管理和项目负责人的岗位&emsp;&emsp;工资可以很直接地反映对工作能力的要求，这四个阶段应该对应了入门-&gt;晋级-&gt;资深-&gt;大牛。看来上岗之后还有很多修行路等着我们岗位信息及其与薪资关系&emsp;&emsp;一个岗位基本的信息包括经验要求、学历要求、所处行业、公司规模和融资情况，接下来将展示两个城市里这5个特征的分布比例，以及他们和薪资之间的关系&emsp;&emsp;通过点击方框来切换要查看的特征，上下两排的按钮效果是一样的经验学历行业规模融资 12 34 经验学历行业规模融资 数据分析岗位关键词&emsp;&emsp;两个城市关于数据分析岗位的关键词以及各自占比比较接近，所以把两个城市的数据合并到一起，让我们一起看一下关于这个岗位有哪些关键词吧～主要关键词：大数据、数据挖掘、金融、带薪年假、绩效奖金、spss、sas、建模、电商、运营、节日礼物等 公司关键词&emsp;&emsp;让我们来看看这些公司主要怎么展示自己的&emsp;&emsp;大概都是薪酬待遇福利、发展前景好，平台大，领导同事牛逼，团队氛围好之类的意思 任职要求关键词&emsp;&emsp;这些是从一大堆任职要求描述里提取的关键词&emsp;&emsp;主要关键词有数据分析、业务、模型、产品、工作经验等&emsp;&emsp;对于我来说，工作经验 优先、相关专业这几个词虽然不大但是异常的刺眼 技能要求词云深圳 广州 &emsp;&emsp;两个城市对工具的需求有点不一样&emsp;&emsp;在广州，excel和sql占了两个很大比例，比其他工具要多不少&emsp;&emsp;但在深圳各个基本工具都占有较大比例，或者说均衡一点，例如python、r、sas、sql、excel，并且python和sql占比最大 &emsp;&emsp;按照’热度’来说，总体排名前10的工具有sql、python、excel、r、sas、spss、hadoop、hive、ppt、spark&emsp;&emsp;看来除了基本的office办公和sql，懂得一门语言应该是标配，能使用分布式工具就更好了 &emsp;&emsp;接下来我们将根据技能展开一些分析，主要是看要求这些技能的岗位的平均工资以及尝试探索6个主要行业对这些技能的需求度技能与薪资&emsp;&emsp;现在我们来看一下这些工具和工资的关系，感受一下它们各自的’价值’&emsp;&emsp;如果想看单个城市的图，可以点相应城市的标签，然后取消”两城市平均”的标签 &emsp;&emsp;两个城市在技能-薪资分布大体比较接近，所以合并到一起求了个平均值，但是要注意，广州的工资普遍要比深圳低3-5k&emsp;&emsp;粗略来看，根据行业工资可以分成3档：&emsp;&emsp;1档： excel、ppt&emsp;&emsp;2档： tableau、spss、mysql、oracle、sql、sas、r、python&emsp;&emsp;3档： hive、linux、java、hadoop、spark&emsp;&emsp;可以感性地判断，excel和ppt是最基础，其次是掌握sql和一门分析语言，最好能做出效果好的可视化，如果想进阶可以学习分布式工具，除此之外linux和java是很好的加分项&emsp;&emsp;不过两个城市还是有一些差别：&emsp;&emsp;深圳的spss工资接近sas、r、python，达到19.7，但是广州的spss就较低，平均13k&emsp;&emsp;广州具体提及sql类型的，例如mysql和oracle的工资较高，一个17.06一个17.96，通过查看对应的任职要求，发现广州明确指出sql类型的，大多写的比较规范，也要求较多是一些知名公司，但是这个情况到深圳就好像不存在了&emsp;&emsp;广州似乎还很看重可视化，要求tableau的岗位工资平均16.36，比python、r这些还要高1k 各行业技能需求&emsp;&emsp;在各个行业里，排名前六的工具是一样的，可以算是行业通用技能了，所以接下来对比一下各个行业对这些技能需求的差别：&emsp;&emsp;分别统计这6个行业里，这些技能在所有技能里的’提及率’(该工具标签出现频率)&emsp;&emsp;提及率越高，说明可能这个行业可能更依赖这个工具，但也不绝对，仅供参考&emsp;&emsp;如果这几个工具的提及率都一般，说明这个行业可能要求使用的工具更杂 &emsp;&emsp;对每个行业来说，sql应该属于必备工具了，提及率都是最高的&emsp;&emsp;和其他行业差别最大的就是金融行业，sas的提及率比其他行业高了很多，金融行业使用sas多可以说是传统了，除此之外其他工具的提及率在其他行业里也算中上水平，其中r、spss、sql在这6个行业里占比最高&emsp;&emsp;第二类是电子商务和企业服务，sql和excel的提及率较其他工具高，电子商务的python、spss和r在所有行业里提及率最低&emsp;&emsp;剩下三个行业，移动互联网、数据服务、游戏，他们在这6个工具的’提及率’上比较接近，除了游戏行业对sas的提及率最低（毕竟sas在金融和医药等行业用的更多，游戏行业感觉和他们’不是一路人’）这3个行业对各个工具感觉是雨露均沾，没有像前三个行业一样对某个工具比其他工具提及的更显著频繁&emsp;&emsp;总体来看，在常用分析工具里，spss可能是用的较少的一个 &emsp;&emsp;除了这6个工具，这6各个行业都有一些自己’专属’的工具：&emsp;&emsp;游戏行业对linux的需求比其他行业要高很多&emsp;&emsp;数据服务对java的需求也比其他行业高很多&emsp;&emsp;数据服务和企业服务对ppt的’提及率’仅次于这6个工具，充分体现了’服务’行业的特色 结尾&emsp;&emsp;&emsp;&emsp;以上分析了一些岗位信息以及它们和薪资之间的关系，相信可以给对不了解数据分析工作的同学提供一个直观的初步认识，做好这些工作之后，我也打算开始找自己的工作了，希望其他和我一样转行的同学们都能找到一个好的起点]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>拉勾网</tag>
        <tag>数据分析岗</tag>
        <tag>echarts</tag>
      </tags>
  </entry>
</search>
